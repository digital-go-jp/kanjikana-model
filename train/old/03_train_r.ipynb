{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# カタカナから漢字・アルファベットを推測するモデルを学習する\n",
    "オープンソース辞書及び，Wikipediaから作成した漢字とカタカナのペアデータを用いて，Transformerで，スクラッチで学習する。\n",
    "\n",
    "入力の文字（カタカナ）は一文字ずつに分割，また，出力の文字（漢字・アルファベットなど）も一文字ずつに分割して，学習する。\n",
    "\n",
    "データセットは，漢字からカタカナを推測するモデルのデータを，漢字とカナを逆にした者を作成して利用する。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ準備\n",
    "データは，[dict](../dict)で作成した姓名辞書データのoss.json及びseimei.jsonと，Wikipediaから抜き出した，漢字姓名とカタカナ姓名（スペース区切りのもの）を用いて，学習に使用した。単語単位である姓名辞書データと，Wikipediaのスペースで区切られた姓名を用いることで，姓名で漢字とカナが入力された場合の学習を可能とした。\n",
    "\n",
    "また，ひらがなとカタカナ，\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スクリプトで使う環境変数をセット\n",
    "import os\n",
    "os.environ['VER']='1.7o'\n",
    "os.environ['DIR']='dataset/dataset.'+os.environ['VER']\n",
    "os.environ['MODEL']='model/model_r.'+os.environ['VER']\n",
    "os.environ[\"LOG\"]='logs/logs_r.'+os.environ['VER']\n",
    "os.environ['RATIO']=\"0.01\"\n",
    "os.environ['WIKI']=\"../dict/oss/wikipedia/wikiname.txt\" # Wikipediaの概要欄から取得した姓名の漢字・アルファベットとカタカナのペアデータ。このファイルの姓名をスペースで区切って丹後他院位にしたものは，辞書ファイルに追加済み\n",
    "os.environ['DICT']='../dict/data/data.'+os.environ['VER']\n",
    "os.makedirs(os.environ['DIR'],exist_ok=True)\n",
    "os.makedirs(os.environ['MODEL'],exist_ok=True)\n",
    "\n",
    "os.makedirs(os.environ['DIR'],exist_ok=True)\n",
    "os.makedirs(os.environ['MODEL'],exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## カタカナと漢字・アルファベットとのペアから学習する\n",
    "学習は，訓練用データを用いて学習し，１エポックごとに開発用データ(valid.json)でLossを確認する。\n",
    "\n",
    "\n",
    "### パラメタ\n",
    "- emb_size \n",
    "入力，出力の文字のエンベッドのDimension\n",
    "\n",
    "- nhead\n",
    "マルチヘッド数\n",
    "\n",
    "- ffn_hid_dim\n",
    "FFNのDimension\n",
    "\n",
    "- num_encoder_layer\n",
    "エンコードレイヤーの数\n",
    "\n",
    "- num_decoder_layer\n",
    "デコードレイヤーの数\n",
    "\n",
    "- lr \n",
    "学習率\n",
    "\n",
    "- dropout\n",
    "ドロップアウトの割合, 0-1\n",
    "\n",
    "- num_epochs\n",
    "何周学習データを用いて学習を行うか\n",
    "\n",
    "- device\n",
    "mps,cpu,cudaから選択\n",
    "-- mps\n",
    "アップルシリコンを搭載したマシンで実行する際に選択\n",
    "-- cuda\n",
    "CUDAが利用できる環境で選択\n",
    "-- cpu\n",
    "上記以外は，CPUモードを選択\n",
    "\n",
    "- earlystop_patient\n",
    "開発用データでlossが下がらなくなってearlystop_patient回計算が終了した場合に，num_epochs数以下でも，計算を終了させる\n",
    "\n",
    "- output_dir\n",
    "モデルの出力ディレクトリ\n",
    "\n",
    "- tensorboard_logdir\n",
    "tensorboard形式のログの出力ディレクトリ。学習状況を確認するためには`tensorboard --logdir logs`を実行後，ブラウザでhttp://localhost:6000/から確認\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "- prefix\n",
    "jsonl形式のデータのprefix\n",
    "\n",
    "- source_lang\n",
    "jsonl形式のデータのsourceのキー\n",
    "\n",
    "- target_lang\n",
    "jsonl形式のデータのtargetのキー\n",
    "\n",
    "- train_file\n",
    "訓練用データのJSONLファイル\n",
    "\n",
    "- valid_file\n",
    "開発用データのJSONLファイル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epochs:145\n",
      "Epoch: 1, Train loss: 4.866, Val loss: 4.029, Epoch time = 1488.752s\n",
      "Epoch: 2, Train loss: 3.684, Val loss: 3.317, Epoch time = 1490.937s\n",
      "Epoch: 3, Train loss: 3.409, Val loss: 3.089, Epoch time = 1487.415s\n",
      "Epoch: 4, Train loss: 3.242, Val loss: 2.878, Epoch time = 1487.076s\n",
      "Epoch: 5, Train loss: 3.097, Val loss: 2.695, Epoch time = 1488.520s\n",
      "Epoch: 6, Train loss: 2.976, Val loss: 2.558, Epoch time = 1487.123s\n",
      "Epoch: 7, Train loss: 2.869, Val loss: 2.409, Epoch time = 1486.147s\n",
      "Epoch: 8, Train loss: 2.771, Val loss: 2.300, Epoch time = 1487.719s\n",
      "Epoch: 9, Train loss: 2.678, Val loss: 2.148, Epoch time = 1489.539s\n",
      "Epoch: 10, Train loss: 2.591, Val loss: 2.050, Epoch time = 1488.656s\n",
      "Epoch: 11, Train loss: 2.509, Val loss: 1.929, Epoch time = 1488.636s\n",
      "Epoch: 12, Train loss: 2.436, Val loss: 1.858, Epoch time = 1490.486s\n",
      "Epoch: 13, Train loss: 2.366, Val loss: 1.777, Epoch time = 1489.655s\n",
      "Epoch: 14, Train loss: 2.302, Val loss: 1.692, Epoch time = 1487.598s\n",
      "Epoch: 15, Train loss: 2.243, Val loss: 1.620, Epoch time = 1488.807s\n",
      "Epoch: 16, Train loss: 2.187, Val loss: 1.558, Epoch time = 1484.643s\n",
      "Epoch: 17, Train loss: 2.134, Val loss: 1.482, Epoch time = 1485.372s\n",
      "Epoch: 18, Train loss: 2.087, Val loss: 1.449, Epoch time = 1487.656s\n",
      "Epoch: 19, Train loss: 2.042, Val loss: 1.390, Epoch time = 1484.313s\n",
      "Epoch: 20, Train loss: 2.002, Val loss: 1.345, Epoch time = 1484.249s\n",
      "Epoch: 21, Train loss: 1.962, Val loss: 1.304, Epoch time = 1485.884s\n",
      "Epoch: 22, Train loss: 1.926, Val loss: 1.278, Epoch time = 1483.764s\n",
      "Epoch: 23, Train loss: 1.892, Val loss: 1.244, Epoch time = 1483.824s\n",
      "Epoch: 24, Train loss: 1.861, Val loss: 1.209, Epoch time = 1486.742s\n",
      "Epoch: 25, Train loss: 1.829, Val loss: 1.180, Epoch time = 1486.338s\n",
      "Epoch: 26, Train loss: 1.800, Val loss: 1.154, Epoch time = 1485.073s\n",
      "Epoch: 27, Train loss: 1.772, Val loss: 1.121, Epoch time = 1483.842s\n",
      "Epoch: 28, Train loss: 1.745, Val loss: 1.098, Epoch time = 1483.853s\n",
      "Epoch: 29, Train loss: 1.720, Val loss: 1.083, Epoch time = 1482.990s\n",
      "Epoch: 30, Train loss: 1.695, Val loss: 1.070, Epoch time = 1482.549s\n",
      "Epoch: 31, Train loss: 1.672, Val loss: 1.047, Epoch time = 1482.980s\n",
      "Epoch: 32, Train loss: 1.649, Val loss: 1.018, Epoch time = 1483.087s\n",
      "Epoch: 33, Train loss: 1.629, Val loss: 0.997, Epoch time = 1483.150s\n",
      "Epoch: 34, Train loss: 1.609, Val loss: 0.985, Epoch time = 1484.056s\n",
      "Epoch: 35, Train loss: 1.588, Val loss: 0.955, Epoch time = 1485.454s\n",
      "Epoch: 36, Train loss: 1.569, Val loss: 0.964, Epoch time = 1481.057s\n",
      "Epoch: 37, Train loss: 1.551, Val loss: 0.938, Epoch time = 1482.626s\n",
      "Epoch: 38, Train loss: 1.533, Val loss: 0.935, Epoch time = 1484.272s\n",
      "Epoch: 39, Train loss: 1.516, Val loss: 0.913, Epoch time = 1485.383s\n",
      "Epoch: 40, Train loss: 1.500, Val loss: 0.889, Epoch time = 1483.671s\n",
      "Epoch: 41, Train loss: 1.483, Val loss: 0.886, Epoch time = 1482.172s\n",
      "Epoch: 42, Train loss: 1.469, Val loss: 0.867, Epoch time = 1485.684s\n",
      "Epoch: 43, Train loss: 1.455, Val loss: 0.860, Epoch time = 1484.964s\n",
      "Epoch: 44, Train loss: 1.441, Val loss: 0.860, Epoch time = 1485.760s\n",
      "Epoch: 45, Train loss: 1.428, Val loss: 0.845, Epoch time = 1485.882s\n",
      "Epoch: 46, Train loss: 1.415, Val loss: 0.840, Epoch time = 1486.668s\n",
      "Epoch: 47, Train loss: 1.403, Val loss: 0.837, Epoch time = 1487.380s\n",
      "Epoch: 48, Train loss: 1.389, Val loss: 0.823, Epoch time = 1488.711s\n",
      "Epoch: 49, Train loss: 1.378, Val loss: 0.807, Epoch time = 1485.545s\n",
      "Epoch: 50, Train loss: 1.367, Val loss: 0.803, Epoch time = 1486.923s\n",
      "Epoch: 51, Train loss: 1.355, Val loss: 0.797, Epoch time = 1489.368s\n",
      "Epoch: 52, Train loss: 1.345, Val loss: 0.830, Epoch time = 1487.904s\n",
      "Epoch: 53, Train loss: 1.334, Val loss: 0.784, Epoch time = 1490.979s\n",
      "Epoch: 54, Train loss: 1.323, Val loss: 0.780, Epoch time = 1490.239s\n",
      "Epoch: 55, Train loss: 1.315, Val loss: 0.767, Epoch time = 1491.017s\n",
      "Epoch: 56, Train loss: 1.304, Val loss: 0.766, Epoch time = 1496.731s\n",
      "Epoch: 57, Train loss: 1.295, Val loss: 0.759, Epoch time = 1497.094s\n",
      "Epoch: 58, Train loss: 1.286, Val loss: 0.754, Epoch time = 1509.243s\n",
      "Epoch: 59, Train loss: 1.276, Val loss: 0.745, Epoch time = 1510.332s\n",
      "Epoch: 60, Train loss: 1.269, Val loss: 0.762, Epoch time = 1498.104s\n",
      "Epoch: 61, Train loss: 1.260, Val loss: 0.744, Epoch time = 1497.895s\n",
      "Epoch: 62, Train loss: 1.253, Val loss: 0.739, Epoch time = 1497.002s\n",
      "Epoch: 63, Train loss: 1.243, Val loss: 0.735, Epoch time = 1494.416s\n",
      "Epoch: 64, Train loss: 1.235, Val loss: 0.729, Epoch time = 1492.141s\n",
      "Epoch: 65, Train loss: 1.228, Val loss: 0.733, Epoch time = 1504.142s\n",
      "Epoch: 66, Train loss: 1.221, Val loss: 0.725, Epoch time = 1491.766s\n",
      "Epoch: 67, Train loss: 1.213, Val loss: 0.722, Epoch time = 1490.889s\n",
      "Epoch: 68, Train loss: 1.207, Val loss: 0.717, Epoch time = 1488.616s\n",
      "Epoch: 69, Train loss: 1.198, Val loss: 0.718, Epoch time = 1488.699s\n",
      "Epoch: 70, Train loss: 1.190, Val loss: 0.715, Epoch time = 1488.394s\n",
      "Epoch: 71, Train loss: 1.183, Val loss: 0.707, Epoch time = 1490.278s\n",
      "Epoch: 72, Train loss: 1.179, Val loss: 0.705, Epoch time = 1489.375s\n",
      "Epoch: 73, Train loss: 1.171, Val loss: 0.695, Epoch time = 1488.948s\n",
      "Epoch: 74, Train loss: 1.163, Val loss: 0.701, Epoch time = 1488.247s\n",
      "Epoch: 75, Train loss: 1.158, Val loss: 0.699, Epoch time = 1490.085s\n",
      "Epoch: 76, Train loss: 1.153, Val loss: 0.693, Epoch time = 1489.188s\n",
      "Epoch: 77, Train loss: 1.146, Val loss: 0.682, Epoch time = 1490.205s\n",
      "Epoch: 78, Train loss: 1.140, Val loss: 0.693, Epoch time = 1489.340s\n",
      "Epoch: 79, Train loss: 1.134, Val loss: 0.697, Epoch time = 1489.468s\n",
      "Epoch: 80, Train loss: 1.128, Val loss: 0.680, Epoch time = 1486.366s\n",
      "Epoch: 81, Train loss: 1.122, Val loss: 0.684, Epoch time = 1488.393s\n",
      "Epoch: 82, Train loss: 1.117, Val loss: 0.684, Epoch time = 1496.502s\n",
      "Epoch: 83, Train loss: 1.111, Val loss: 0.675, Epoch time = 1484.516s\n",
      "Epoch: 84, Train loss: 1.106, Val loss: 0.676, Epoch time = 1485.403s\n",
      "Epoch: 85, Train loss: 1.100, Val loss: 0.675, Epoch time = 1484.494s\n",
      "Epoch: 86, Train loss: 1.095, Val loss: 0.674, Epoch time = 1484.722s\n",
      "Epoch: 87, Train loss: 1.090, Val loss: 0.671, Epoch time = 1493.921s\n",
      "Epoch: 88, Train loss: 1.085, Val loss: 0.674, Epoch time = 1485.537s\n",
      "Epoch: 89, Train loss: 1.080, Val loss: 0.671, Epoch time = 1486.021s\n",
      "Epoch: 90, Train loss: 1.075, Val loss: 0.667, Epoch time = 1484.697s\n",
      "Epoch: 91, Train loss: 1.071, Val loss: 0.693, Epoch time = 1484.323s\n",
      "Epoch: 92, Train loss: 1.065, Val loss: 0.670, Epoch time = 1486.165s\n",
      "Epoch: 93, Train loss: 1.061, Val loss: 0.658, Epoch time = 1485.888s\n",
      "Epoch: 94, Train loss: 1.056, Val loss: 0.659, Epoch time = 1481.719s\n",
      "Epoch: 95, Train loss: 1.053, Val loss: 0.662, Epoch time = 1482.302s\n",
      "Epoch: 96, Train loss: 1.046, Val loss: 0.657, Epoch time = 1480.930s\n",
      "Epoch: 97, Train loss: 1.043, Val loss: 0.662, Epoch time = 1481.346s\n",
      "Epoch: 98, Train loss: 1.038, Val loss: 0.657, Epoch time = 1480.414s\n",
      "Epoch: 99, Train loss: 1.034, Val loss: 0.658, Epoch time = 1483.208s\n",
      "Epoch: 100, Train loss: 1.029, Val loss: 0.648, Epoch time = 1482.335s\n",
      "Epoch: 101, Train loss: 1.025, Val loss: 0.649, Epoch time = 1478.417s\n",
      "Epoch: 102, Train loss: 1.022, Val loss: 0.655, Epoch time = 1480.489s\n",
      "Epoch: 103, Train loss: 1.017, Val loss: 0.649, Epoch time = 1484.528s\n",
      "Epoch: 104, Train loss: 1.014, Val loss: 0.648, Epoch time = 1486.033s\n"
     ]
    }
   ],
   "source": [
    "# 訓練実行\n",
    "\n",
    "!python ./char_model.py \\\n",
    "  --emb_size 512 \\\n",
    "  --nhead 8 \\\n",
    "  --ffn_hid_dim 2048 \\\n",
    "  --batch_size 32 \\\n",
    "  --num_encoder_layers 8 \\\n",
    "  --num_decoder_layers 8 \\\n",
    "  --lr 0.00002 \\\n",
    "  --dropout 0.3 \\\n",
    "  --num_epochs 145 \\\n",
    "  --device cuda  \\\n",
    "  --earlystop_patient 3 \\\n",
    "  --output_dir $MODEL \\\n",
    "  --tensorboard_logdir $LOG \\\n",
    "  --prefix translation \\\n",
    "  --source_lang kana \\\n",
    "  --target_lang kanji \\\n",
    "  --train_file $DIR/train.jsonl \\\n",
    "  --valid_file $DIR/valid.jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c1623028950fb930\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c1623028950fb930\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルを検証する\n",
    "作成したモデルを，検証用データを用いて検証し，正解率を算出する\n",
    "\n",
    "\n",
    "## パラメタ\n",
    "\n",
    "- test_file    \n",
    "jsonl形式の検証用データファイル\n",
    "\n",
    "- model_file    \n",
    "transformer_model.pyで作成したモデルファイル。\n",
    "\n",
    "- outfile    \n",
    "検証結果を格納するファイル\n",
    "\n",
    "- device    \n",
    "cpu限定\n",
    "\n",
    "- nbest    \n",
    "検証用データの漢字姓名を入力データとした時に，モデルが出力するカタカナ姓名を確率の高い方からnbest個出力する。beam_width以下。searchパラメタがbothかbeamの時有効\n",
    "\n",
    "- beam_width    \n",
    "ビームサーチのビーム幅。searchパラメタがbothかbeamの時有効\n",
    "\n",
    "- max_len    \n",
    "出力するカタカナ姓名の最大長さ\n",
    "\n",
    "- search    \n",
    "greedy,beam,bothから選択    \n",
    "  - greedy    \n",
    "貪欲法による探索    \n",
    "  - beam    \n",
    "ビームサーチによる探索    \n",
    "  - both    \n",
    "貪欲砲，ビームサーチ両方の探索    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検証用データを実行\n",
    "\n",
    "\n",
    "!python generate_batch.py \\\n",
    "  --test_file $DIR/test.jsonl \\\n",
    "  --model_file $MODEL/checkpoint_best.pt \\\n",
    "  --outfile $DIR/generate_r.txt \\\n",
    "  --device cpu \\\n",
    "  --nbest 5 \\\n",
    "  --beam_width 5 \\\n",
    "  --max_len 100 \\\n",
    "  --search greedy\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc=0.550316273720529\n"
     ]
    }
   ],
   "source": [
    "# 検証結果を評価\n",
    "import pandas as pd\n",
    "import os\n",
    "dr=os.environ[\"DIR\"]\n",
    "df = pd.read_csv(f'{dr}/generate.txt',encoding='utf-8',sep=\"\\t\")\n",
    "tgt = df['tgt'].tolist()\n",
    "pred = df['pred'].tolist()\n",
    "ok=0\n",
    "for t,p in zip(tgt,pred):\n",
    "    if t == p:\n",
    "        ok+=1\n",
    "\n",
    "print(f'acc={ok/len(tgt)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルをJavaのDJL用に変換する\n",
    "Pytorchで学習したモデルをJavaで使えるようにモデルの変換を行う。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カナから漢字\n",
    "!python convert_jitscript.py  \\\n",
    "    --model_file=$MODEL/checkpoint_best.pt \\\n",
    "    --model_script=$MODEL/script_$VER.pt \\\n",
    "    --encoder=$MODEL/encoder_$VER.pt \\\n",
    "    --decoder=$MODEL/decoder_$VER.pt \\\n",
    "    --positional_encoding=$MODEL/positional_encoding_$VER.pt \\\n",
    "    --generator=$MODEL/generator_$VER.pt \\\n",
    "    --src_tok_emb=$MODEL/src_tok_emb_$VER.pt \\\n",
    "    --tgt_tok_emb=$MODEL/tgt_tok_emb_$VER.pt \\\n",
    "    --vocab_src=$MODEL/vocab_src_$VER.txt \\\n",
    "    --vocab_tgt=$MODEL/vocab_tgt_$VER.txt \\\n",
    "    --params=$MODEL/params_$VER.json \\\n",
    "    --source_lang=kana \\\n",
    "    --target_lang=kanji \\\n",
    "    --device=cpu\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
