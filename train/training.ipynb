{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 漢字・アルファベットからカタカナを推測するモデルを学習する\n",
    "オープンソース辞書及び，Wikipediaから作成した漢字とカタカナのペアデータを用いて，Transformer+Seq2Seqのモデルで，スクラッチで学習する。\n",
    "\n",
    "入力の文字（漢字・アルファベットなど）は一文字ずつに分割，また，出力の文字（カタカナ）も一文字ずつに分割して，学習する。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ準備\n",
    "データは，[dict](../dict)で作成した姓名辞書データのoss.json及びseimei.jsonと，Wikipediaから抜き出した，漢字姓名とカタカナ姓名（スペース区切りのもの）を用いて，学習に使用した。単語単位である姓名辞書データと，Wikipediaのスペースで区切られた姓名を用いることで，姓名で漢字とカナが入力された場合の学習を可能とした。\n",
    "\n",
    "また，ひらがなとカタカナ，\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スクリプトで使う環境変数をセット\n",
    "import os\n",
    "os.environ['DIR']='dataset'\n",
    "os.environ['MODEL']='model'\n",
    "os.environ[\"LOG\"]='logs'\n",
    "os.environ['RATIO']=\"0.01\"\n",
    "os.environ['WIKI']=\"../dict/wikipedia/wikiname.txt\"\n",
    "\n",
    "os.makedirs(os.environ['DIR'],exist_ok=True)\n",
    "os.makedirs(os.environ['MODEL'],exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"dic1\": \"../dict/oss.json\",\n",
      "  \"dic2\": \"../dict/seimei.json\",\n",
      "  \"outfile\": \"tmp.json\"\n",
      "}\n",
      "len=1620498\n",
      "cnt=1673683\n"
     ]
    }
   ],
   "source": [
    "# オープンソース辞書とWikipediaから作成した単語辞書をマージする\n",
    "!python mergedic.py --dic1 ../dict/oss.json --dic2 ../dict/seimei.json --outfile tmp.json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"json\": \"tmp.json\",\n",
      "  \"outdir\": \"dataset\",\n",
      "  \"ratio\": 0.01\n",
      "}\n",
      "k_max_len=72\n",
      "v_max_len=120\n",
      "train,1640211\n",
      "valid,16736\n",
      "test,16736\n"
     ]
    }
   ],
   "source": [
    "# 作成したデータを訓練用，開発用，検証用に分ける。開発用と検証用がそれぞれ全体の0.01\n",
    "!python prep.py --json tmp.json --outdir $DIR --ratio $RATIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikipediaの漢字姓名とカナ姓名を取得し，訓練，開発，検証用に分ける。\n",
    "!python catawk.py --begin_idx_per 0 --end_idx_per 1 --appendfile $DIR/valid.src --index 0 --infile $WIKI --infile_delimiter tsv\n",
    "!python catawk.py --begin_idx_per 0 --end_idx_per 1 --appendfile $DIR/valid.tgt --index 1 --infile $WIKI --infile_delimiter tsv\n",
    "!python catawk.py --begin_idx_per 1 --end_idx_per 2 --appendfile $DIR/test.src --index 0 --infile $WIKI --infile_delimiter tsv\n",
    "!python catawk.py --begin_idx_per 1 --end_idx_per 2 --appendfile $DIR/test.tgt --index 1 --infile $WIKI --infile_delimiter tsv\n",
    "!python catawk.py --begin_idx_per 2 --end_idx_per 100 --appendfile $DIR/train.src --index 0 --infile $WIKI --infile_delimiter tsv\n",
    "!python catawk.py --begin_idx_per 2 --end_idx_per 100 --appendfile $DIR/train.tgt --index 1 --infile $WIKI --infile_delimiter tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"outfile\": \"dataset/hirakata.txt\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ひらがなとカタカナのペアデータを作成し，訓練データに混ぜる\n",
    "!python kana.py --outfile $DIR/hirakata.txt\n",
    "\n",
    "!python catawk.py --begin_idx_per 0 --end_idx_per 100 --appendfile $DIR/train.src --index 0 --infile $DIR/hirakata.txt\n",
    "!python catawk.py --begin_idx_per 0 --end_idx_per 100 --appendfile $DIR/train.tgt --index 1 --infile $DIR/hirakata.txt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"jsonfile\": \"../dict/tankanji.json\",\n",
      "  \"outfile\": \"tankanji.txt\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 単漢字とカタカナのデータを，訓練データに混ぜる \n",
    "!python jsontotext.py --jsonfile ../dict/tankanji.json --outfile tankanji.txt\n",
    "\n",
    "!python catawk.py --begin_idx_per 0 --end_idx_per 100 --appendfile $DIR/train.src --index 0 --infile tankanji.txt\n",
    "!python catawk.py --begin_idx_per 0 --end_idx_per 100 --appendfile $DIR/train.tgt --index 1 --infile tankanji.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"train_src\": \"dataset/train.src\",\n",
      "  \"train_tgt\": \"dataset/train.tgt\",\n",
      "  \"test_src\": \"dataset/test.src\",\n",
      "  \"test_tgt\": \"dataset/test.tgt\",\n",
      "  \"valid_src\": \"dataset/valid.src\",\n",
      "  \"valid_tgt\": \"dataset/valid.tgt\"\n",
      "}\n",
      "train=1924400\n",
      "omit=508\n"
     ]
    }
   ],
   "source": [
    "# 開発，検証用に，訓練データに入っているものがあれば，削除する(インサンプルを阻止)\n",
    "!python omit_testval.py --train_src $DIR/train.src --train_tgt $DIR/train.tgt --test_src $DIR/test.src --test_tgt $DIR/test.tgt --valid_src $DIR/valid.src --valid_tgt $DIR/valid.tgt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"tgt\": \"dataset/train.tgt\",\n",
      "  \"src\": \"dataset/train.src\"\n",
      "}\n",
      "{\n",
      "  \"tgt\": \"dataset/valid.tgt\",\n",
      "  \"src\": \"dataset/valid.src\"\n",
      "}\n",
      "{\n",
      "  \"tgt\": \"dataset/test.tgt\",\n",
      "  \"src\": \"dataset/test.src\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 訓練，開発，検証データをシャッフルする\n",
    "\n",
    "!python shuffle.py --src $DIR/train.src --tgt $DIR/train.tgt\n",
    "!python shuffle.py --src $DIR/valid.src --tgt $DIR/valid.tgt\n",
    "!python shuffle.py --src $DIR/test.src --tgt $DIR/test.tgt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"infile\": \"dataset/train.src\",\n",
      "  \"outfile\": \"dataset/train.src\"\n",
      "}\n",
      "{\n",
      "  \"infile\": \"dataset/train.tgt\",\n",
      "  \"outfile\": \"dataset/train.tgt\"\n",
      "}\n",
      "{\n",
      "  \"infile\": \"dataset/valid.src\",\n",
      "  \"outfile\": \"dataset/valid.src\"\n",
      "}\n",
      "{\n",
      "  \"infile\": \"dataset/valid.tgt\",\n",
      "  \"outfile\": \"dataset/valid.tgt\"\n",
      "}\n",
      "{\n",
      "  \"infile\": \"dataset/test.src\",\n",
      "  \"outfile\": \"dataset/test.src\"\n",
      "}\n",
      "{\n",
      "  \"infile\": \"dataset/test.tgt\",\n",
      "  \"outfile\": \"dataset/test.tgt\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# データをスペース区切りにする（文字単位で，学習させるため）\n",
    "\n",
    "!python space.py --infile $DIR/train.src --outfile $DIR/train.src\n",
    "!python space.py --infile $DIR/train.tgt --outfile $DIR/train.tgt\n",
    "!python space.py --infile $DIR/valid.src --outfile $DIR/valid.src\n",
    "!python space.py --infile $DIR/valid.tgt --outfile $DIR/valid.tgt\n",
    "!python space.py --infile $DIR/test.src --outfile $DIR/test.src\n",
    "!python space.py --infile $DIR/test.tgt --outfile $DIR/test.tgt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsonl形式に変換する\n",
    "\n",
    "!python format.py --src $DIR/train.src --tgt $DIR/train.tgt --outfile $DIR/train.jsonl\n",
    "!python format.py --src $DIR/test.src --tgt $DIR/test.tgt --outfile $DIR/test.jsonl\n",
    "!python format.py --src $DIR/valid.src --tgt $DIR/valid.tgt --outfile $DIR/valid.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 漢字・アルファベットとカタカナのペアから学習する\n",
    "学習は，訓練用データを用いて学習し，１エポックごとに開発用データ(valid.json)でLossを確認する。\n",
    "\n",
    "\n",
    "### パラメタ\n",
    "- emb_size \n",
    "入力，出力の文字のエンベッドのDimension\n",
    "\n",
    "- nhead\n",
    "マルチヘッド数\n",
    "\n",
    "- ffn_hid_dim\n",
    "FFNのDimension\n",
    "\n",
    "- num_encoder_layer\n",
    "エンコードレイヤーの数\n",
    "\n",
    "- num_decoder_layer\n",
    "デコードレイヤーの数\n",
    "\n",
    "- lr \n",
    "学習率\n",
    "\n",
    "- dropout\n",
    "ドロップアウトの割合, 0-1\n",
    "\n",
    "- num_epochs\n",
    "何周学習データを用いて学習を行うか\n",
    "\n",
    "- device\n",
    "mps,cpu,cudaから選択\n",
    "-- mps\n",
    "アップルシリコンを搭載したマシンで実行する際に選択\n",
    "-- cuda\n",
    "CUDAが利用できる環境で選択\n",
    "-- cpu\n",
    "上記以外は，CPUモードを選択\n",
    "\n",
    "- earlystop_patient\n",
    "開発用データでlossが下がらなくなってearlystop_patient回計算が終了した場合に，num_epochs数以下でも，計算を終了させる\n",
    "\n",
    "- output_dir\n",
    "モデルの出力ディレクトリ\n",
    "\n",
    "- tensorboard_logdir\n",
    "tensorboard形式のログの出力ディレクトリ。学習状況を確認するためには`tensorboard --logdir logs`を実行後，ブラウザでhttp://localhost:6000/から確認\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "- prefix\n",
    "jsonl形式のデータのprefix\n",
    "\n",
    "- source_lang\n",
    "jsonl形式のデータのsourceのキー\n",
    "\n",
    "- target_lang\n",
    "jsonl形式のデータのtargetのキー\n",
    "\n",
    "- train_file\n",
    "訓練用データのJSONLファイル\n",
    "\n",
    "- valid_file\n",
    "開発用データのJSONLファイル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/utsubo-katsuhiko/Documents/GitHub/kanjikana-model/venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "num_epochs:50\n",
      "/Users/utsubo-katsuhiko/Documents/GitHub/kanjikana-model/venv/lib/python3.12/site-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/utsubo-katsuhiko/Documents/GitHub/kanjikana-model/train/./transformer_model.py\", line 509, in <module>\n",
      "    main()\n",
      "  File \"/Users/utsubo-katsuhiko/Documents/GitHub/kanjikana-model/train/./transformer_model.py\", line 505, in main\n",
      "    KanjiKanaTransformer(args).train()\n",
      "  File \"/Users/utsubo-katsuhiko/Documents/GitHub/kanjikana-model/train/./transformer_model.py\", line 450, in train\n",
      "    train_loss = self.train_epoch(train_iter, transformer, optimizer, loss_fn)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/utsubo-katsuhiko/Documents/GitHub/kanjikana-model/train/./transformer_model.py\", line 261, in train_epoch\n",
      "    optimizer.step()\n",
      "  File \"/Users/utsubo-katsuhiko/Documents/GitHub/kanjikana-model/venv/lib/python3.12/site-packages/torch/optim/optimizer.py\", line 487, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/utsubo-katsuhiko/Documents/GitHub/kanjikana-model/venv/lib/python3.12/site-packages/torch/optim/optimizer.py\", line 91, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/utsubo-katsuhiko/Documents/GitHub/kanjikana-model/venv/lib/python3.12/site-packages/torch/optim/adam.py\", line 223, in step\n",
      "    adam(\n",
      "  File \"/Users/utsubo-katsuhiko/Documents/GitHub/kanjikana-model/venv/lib/python3.12/site-packages/torch/optim/optimizer.py\", line 154, in maybe_fallback\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/utsubo-katsuhiko/Documents/GitHub/kanjikana-model/venv/lib/python3.12/site-packages/torch/optim/adam.py\", line 784, in adam\n",
      "    func(\n",
      "  File \"/Users/utsubo-katsuhiko/Documents/GitHub/kanjikana-model/venv/lib/python3.12/site-packages/torch/optim/adam.py\", line 432, in _single_tensor_adam\n",
      "    param.addcdiv_(exp_avg, denom, value=-step_size)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# 訓練実行\n",
    "\n",
    "!python ./transformer_model.py \\\n",
    "  --emb_size 512 \\\n",
    "  --nhead 8 \\\n",
    "  --ffn_hid_dim 2048 \\\n",
    "  --batch_size 32 \\\n",
    "  --num_encoder_layers 8 \\\n",
    "  --num_decoder_layers 8 \\\n",
    "  --lr 0.00002 \\\n",
    "  --dropout 0.3 \\\n",
    "  --num_epochs 50 \\\n",
    "  --device cuda \\\n",
    "  --earlystop_patient 3 \\\n",
    "  --output_dir $MODEL \\\n",
    "  --tensorboard_logdir $LOG \\\n",
    "  --prefix translation \\\n",
    "  --source_lang kanji \\\n",
    "  --target_lang kana \\\n",
    "  --train_file $DIR/train.jsonl \\\n",
    "  --valid_file $DIR/valid.jsonl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルを検証する\n",
    "作成したモデルを，検証用データを用いて検証し，正解率を算出する\n",
    "\n",
    "\n",
    "## パラメタ\n",
    "\n",
    "- test_file\n",
    "jsonl形式の検証用データファイル\n",
    "\n",
    "- model_file\n",
    "transformer_model.pyで作成したモデルファイル。\n",
    "\n",
    "- outfile\n",
    "検証結果を格納するファイル\n",
    "\n",
    "- device\n",
    "cpu限定\n",
    "\n",
    "-nbest\n",
    "検証用データの漢字姓名を入力データとした時に，モデルが出力するカタカナ姓名を確率の高い方からnbest個出力する。beam_width以下。searchパラメタがbothかbeamの時有効\n",
    "\n",
    "- beam_width\n",
    "ビームサーチのビーム幅。searchパラメタがbothかbeamの時有効\n",
    "\n",
    "- max_len\n",
    "出力するカタカナ姓名の最大長さ\n",
    "\n",
    "- search\n",
    "greedy,beam,bothから選択\n",
    "-- greedy\n",
    "貪欲法による探索\n",
    "-- beam\n",
    "ビームサーチによる探索\n",
    "-- both\n",
    "貪欲砲，ビームサーチ両方の探索\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検証用データを実行\n",
    "\n",
    "\n",
    "!python generate_batch.py \\\n",
    "  --test_file $DIR/test.jsonl \\\n",
    "  --model_file $MODEL/checkpoint_best.pt \\\n",
    "  --outfile $DIR/generate.txt \\\n",
    "  --device cpu \\\n",
    "  --nbest 5 \\\n",
    "  --beam_width 5 \\\n",
    "  --max_len 100 \\\n",
    "  --search both\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検証結果を評価\n",
    "!python evaluate.py --infile $DIR/generate.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# カタカナから漢字を推測するモデルを学習する\n",
    "漢字からカタカナを推測するモデルと同様に，オープンソース辞書及び，Wikipediaから作成した漢字とカタカナのペアデータを用いて，Transformer+Seq2Seqのモデルで，スクラッチで学習する。\n",
    "\n",
    "入力の文字（カタカナ）は一文字ずつに分割，また，出力の文字（漢字・アルファベットなど）も一文字ずつに分割して，学習する。\n",
    "\n",
    "データセットは，漢字からカタカナを推測するモデルのデータを，漢字とカナを逆にした者を作成して利用する。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 環境変数とディレクトリを作成，及びデータのコピー\n",
    "import os\n",
    "import shutil\n",
    "os.environ['DIR_R']='dataset_r'\n",
    "os.environ['MODEL_R']='model_r'\n",
    "os.environ['LOG_R']='logs_r'\n",
    "\n",
    "os.makedirs(os.environ['DIR_R'],exist_ok=True)\n",
    "os.makedirs(os.environ['MODEL_R'],exist_ok=True)\n",
    "\n",
    "# 漢字からカナ用に作成したデータを，SRCとTGTを逆にコピーする\n",
    "shutil.copy(os.environ['DIR']+\"/train.src\",os.environ['DIR_R']+\"/train.tgt\")\n",
    "shutil.copy(os.environ['DIR']+\"/train.tgt\",os.environ['DIR_R']+\"/train.src\")\n",
    "shutil.copy(os.environ['DIR']+\"/valid.src\",os.environ['DIR_R']+\"/valid.tgt\")\n",
    "shutil.copy(os.environ['DIR']+\"/valid.tgt\",os.environ['DIR_R']+\"/valid.src\")\n",
    "shutil.copy(os.environ['DIR']+\"/test.src\",os.environ['DIR_R']+\"/test.tgt\")\n",
    "shutil.copy(os.environ['DIR']+\"/test.tgt\",os.environ['DIR_R']+\"/test.src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsonl形式に変換する\n",
    "\n",
    "!python format.py --src $DIR_R/train.src --tgt $DIR_R/train.tgt --outfile $DIR_R/train.jsonl\n",
    "!python format.py --src $DIR_R/test.src --tgt $DIR_R/test.tgt --outfile $DIR_R/test.jsonl\n",
    "!python format.py --src $DIR_R/valid.src --tgt $DIR_R/valid.tgt --outfile $DIR_R/valid.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 漢字・アルファベットとカタカナのペアから学習する\n",
    "学習は，訓練用データを用いて学習し，１エポックごとに開発用データ(valid.json)でLossを確認する。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./transformer_model.py \\\n",
    "  --emb_size 512 \\\n",
    "  --nhead 8 \\\n",
    "  --ffn_hid_dim 2048 \\\n",
    "  --batch_size 32 \\\n",
    "  --num_encoder_layers 8 \\\n",
    "  --num_decoder_layers 8 \\\n",
    "  --lr 0.00002 \\\n",
    "  --dropout 0.3 \\\n",
    "  --num_epochs 50 \\\n",
    "  --device mps \\\n",
    "  --earlystop_patient 3 \\\n",
    "  --output_dir $MODEL_R \\\n",
    "  --tensorboard_logdir $LOG_R \\\n",
    "  --prefix translation \\\n",
    "  --source_lang kanji \\\n",
    "  --target_lang kana \\\n",
    "  --train_file $DIR_R/train.jsonl \\\n",
    "  --valid_file $DIR_R/valid.jsonl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルを検証する\n",
    "作成したモデルを，検証用データを用いて検証し，正解率を算出する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検証用データを実行\n",
    "\n",
    "\n",
    "!python generate_batch.py \\\n",
    "  --test_file $DIR_R/test.jsonl \\\n",
    "  --model_file $MODEL_R/checkpoint_best.pt \\\n",
    "  --outfile $DIR_R/generate.txt \\\n",
    "  --device cpu \\\n",
    "  --nbest 5 \\\n",
    "  --beam_width 5 \\\n",
    "  --max_len 100 \\\n",
    "  --search both\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検証結果を評価\n",
    "!python evaluate.py --infile $DIR_R/generate.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルをJavaのDJL用に変換する\n",
    "Pytorchで学習したモデルをJavaで使えるようにモデルの変換を行う。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 漢字からカナ\n",
    "!python convert_jitscript.py  \\\n",
    "    --model_file=$MODEL/checkpoint_best.pt \\\n",
    "    --model_script=$MODEL/script.pt \\\n",
    "    --encoder=$MODEL/encoder.pt \\\n",
    "    --decoder=$MODEL/decoder.pt \\\n",
    "    --positional_encoding=$MODEL/positional_encoding.pt \\\n",
    "    --generator=$MODEL/generator.pt \\\n",
    "    --src_tok_emb=$MODEL/src_tok_emb.pt \\\n",
    "    --tgt_tok_emb=$MODEL/tgt_tok_emb.pt \\\n",
    "    --vocab_src=$MODEL/vocab_src.txt \\\n",
    "    --vocab_tgt=$MODEL/vocab_tgt.txt \\\n",
    "    --params=$MODEL/params.json \\\n",
    "    --device=cpu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カナから漢字\n",
    "!python convert_jitscript.py  \\\n",
    "    --model_file=$MODEL_R/checkpoint_best.pt \\\n",
    "    --model_script=$MODEL_R/script.pt \\\n",
    "    --encoder=$MODEL_R/encoder.pt \\\n",
    "    --decoder=$MODEL_R/decoder.pt \\\n",
    "    --positional_encoding=$MODEL_R/positional_encoding.pt \\\n",
    "    --generator=$MODEL_R/generator.pt \\\n",
    "    --src_tok_emb=$MODEL_R/src_tok_emb.pt \\\n",
    "    --tgt_tok_emb=$MODEL_R/tgt_tok_emb.pt \\\n",
    "    --vocab_src=$MODEL_R/vocab_src.txt \\\n",
    "    --vocab_tgt=$MODEL_R/vocab_tgt.txt \\\n",
    "    --params=$MODEL_R/params.json \\\n",
    "    --device=cpu\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
