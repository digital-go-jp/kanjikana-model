{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 漢字・アルファベットからカタカナを推測するモデルを学習する\n",
    "オープンソース辞書及び，Wikipediaから作成した漢字とカタカナのペアデータを用いて，Transformer+Seq2Seqのモデルで，スクラッチで学習する。\n",
    "\n",
    "入力の文字（漢字・アルファベットなど）は一文字ずつに分割，また，出力の文字（カタカナ）も一文字ずつに分割して，学習する。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ準備\n",
    "データは，[dict](../dict)で作成した姓名辞書データのoss.json及びseimei.jsonと，Wikipediaから抜き出した，漢字姓名とカタカナ姓名（スペース区切りのもの）を用いて，学習に使用した。単語単位である姓名辞書データと，Wikipediaのスペースで区切られた姓名を用いることで，姓名で漢字とカナが入力された場合の学習を可能とした。\n",
    "\n",
    "また，ひらがなとカタカナ，\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スクリプトで使う環境変数をセット\n",
    "import os\n",
    "ver=\"1.7o\"\n",
    "os.environ['ver']=ver\n",
    "os.environ['DIR']=f'dataset.{ver}'\n",
    "os.environ['MODEL']=f'model.{ver}'\n",
    "os.environ[\"LOG\"]=f'logs.{ver}'\n",
    "os.environ['RATIO']=\"0.01\"\n",
    "os.environ['WIKI']=\"../dict/wikipedia/wikiname.txt\" # Wikipediaの概要欄から取得した姓名の漢字・アルファベットとカタカナのペアデータ。このファイルの姓名をスペースで区切って丹後他院位にしたものは，辞書ファイルに追加済み\n",
    "os.environ['DICT']=f'../dict/data/data.{ver}'\n",
    "os.makedirs(os.environ['DIR'],exist_ok=True)\n",
    "os.makedirs(os.environ['MODEL'],exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"dic1\": \"../dict/data/data.1.7o/oss_1.7o.json\",\n",
      "  \"dic2\": \"../dict/data/data.1.7o/seimei_1.7o.json\",\n",
      "  \"outfile\": \"tmp.json\"\n",
      "}\n",
      "len=2070584\n",
      "cnt=2185738\n"
     ]
    }
   ],
   "source": [
    "# オープンソース辞書とWikipediaから作成した単語辞書をマージする\n",
    "!python mergedic.py --dic1 $DICT/oss_$ver.json --dic2 $DICT/seimei_$ver.json --outfile tmp.json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"json\": \"tmp.json\",\n",
      "  \"outdir\": \"dataset.1.7o\",\n",
      "  \"ratio\": 0.01,\n",
      "  \"reverse\": false\n",
      "}\n",
      "k_max_len=72\n",
      "v_max_len=120\n",
      "train,2142024\n",
      "valid,21857\n",
      "test,21857\n"
     ]
    }
   ],
   "source": [
    "# 作成したデータを訓練用，開発用，検証用に分ける。開発用と検証用がそれぞれ全体の0.01\n",
    "!python prep.py --json tmp.json --outdir $DIR --ratio $RATIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikipediaの漢字姓名とカナ姓名を取得し，訓練，開発，検証用に分けて，辞書から作成したデータに追加する。\n",
    "!python catawk.py --begin_idx_per 0 --end_idx_per 1 --appendfile $DIR/valid.src --index 0 --infile $WIKI --infile_delimiter tsv\n",
    "!python catawk.py --begin_idx_per 0 --end_idx_per 1 --appendfile $DIR/valid.tgt --index 1 --infile $WIKI --infile_delimiter tsv\n",
    "!python catawk.py --begin_idx_per 1 --end_idx_per 2 --appendfile $DIR/test.src --index 0 --infile $WIKI --infile_delimiter tsv\n",
    "!python catawk.py --begin_idx_per 1 --end_idx_per 2 --appendfile $DIR/test.tgt --index 1 --infile $WIKI --infile_delimiter tsv\n",
    "!python catawk.py --begin_idx_per 2 --end_idx_per 100 --appendfile $DIR/train.src --index 0 --infile $WIKI --infile_delimiter tsv\n",
    "!python catawk.py --begin_idx_per 2 --end_idx_per 100 --appendfile $DIR/train.tgt --index 1 --infile $WIKI --infile_delimiter tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"outfile\": \"dataset.1.7o/hirakata.txt\",\n",
      "  \"reverse\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ひらがなとカタカナのペアデータを作成し，訓練データに混ぜる\n",
    "!python kana.py --outfile $DIR/hirakata.txt\n",
    "\n",
    "!python catawk.py --begin_idx_per 0 --end_idx_per 100 --appendfile $DIR/train.src --index 0 --infile $DIR/hirakata.txt\n",
    "!python catawk.py --begin_idx_per 0 --end_idx_per 100 --appendfile $DIR/train.tgt --index 1 --infile $DIR/hirakata.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"jsonfile\": \"../dict/tankanji.json\",\n",
      "  \"outfile\": \"tankanji.txt\",\n",
      "  \"reverse\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 単漢字とカタカナのデータを，訓練データに混ぜる \n",
    "!python jsontotext.py --jsonfile ../dict/tankanji.json --outfile tankanji.txt\n",
    "\n",
    "!python catawk.py --begin_idx_per 0 --end_idx_per 100 --appendfile $DIR/train.src --index 0 --infile tankanji.txt\n",
    "!python catawk.py --begin_idx_per 0 --end_idx_per 100 --appendfile $DIR/train.tgt --index 1 --infile tankanji.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"train_src\": \"dataset.1.7o/train.src\",\n",
      "  \"train_tgt\": \"dataset.1.7o/train.tgt\",\n",
      "  \"test_src\": \"dataset.1.7o/test.src\",\n",
      "  \"test_tgt\": \"dataset.1.7o/test.tgt\",\n",
      "  \"valid_src\": \"dataset.1.7o/valid.src\",\n",
      "  \"valid_tgt\": \"dataset.1.7o/valid.tgt\"\n",
      "}\n",
      "train=2429187\n",
      "omit=531\n"
     ]
    }
   ],
   "source": [
    "# 開発，検証用に，訓練データに入っているものがあれば，削除する(インサンプルを阻止)\n",
    "!python omit_testval.py --train_src $DIR/train.src --train_tgt $DIR/train.tgt --test_src $DIR/test.src --test_tgt $DIR/test.tgt --valid_src $DIR/valid.src --valid_tgt $DIR/valid.tgt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"tgt\": \"dataset.1.7o/train.tgt\",\n",
      "  \"src\": \"dataset.1.7o/train.src\"\n",
      "}\n",
      "{\n",
      "  \"tgt\": \"dataset.1.7o/valid.tgt\",\n",
      "  \"src\": \"dataset.1.7o/valid.src\"\n",
      "}\n",
      "{\n",
      "  \"tgt\": \"dataset.1.7o/test.tgt\",\n",
      "  \"src\": \"dataset.1.7o/test.src\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 訓練，開発，検証データをシャッフルする\n",
    "\n",
    "!python shuffle.py --src $DIR/train.src --tgt $DIR/train.tgt\n",
    "!python shuffle.py --src $DIR/valid.src --tgt $DIR/valid.tgt\n",
    "!python shuffle.py --src $DIR/test.src --tgt $DIR/test.tgt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"infile\": \"dataset.1.7o/train.src\",\n",
      "  \"outfile\": \"dataset.1.7o/train.src\"\n",
      "}\n",
      "{\n",
      "  \"infile\": \"dataset.1.7o/train.tgt\",\n",
      "  \"outfile\": \"dataset.1.7o/train.tgt\"\n",
      "}\n",
      "{\n",
      "  \"infile\": \"dataset.1.7o/valid.src\",\n",
      "  \"outfile\": \"dataset.1.7o/valid.src\"\n",
      "}\n",
      "{\n",
      "  \"infile\": \"dataset.1.7o/valid.tgt\",\n",
      "  \"outfile\": \"dataset.1.7o/valid.tgt\"\n",
      "}\n",
      "{\n",
      "  \"infile\": \"dataset.1.7o/test.src\",\n",
      "  \"outfile\": \"dataset.1.7o/test.src\"\n",
      "}\n",
      "{\n",
      "  \"infile\": \"dataset.1.7o/test.tgt\",\n",
      "  \"outfile\": \"dataset.1.7o/test.tgt\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# データをスペース区切りにする（文字単位で，学習させるため）\n",
    "\n",
    "!python space.py --infile $DIR/train.src --outfile $DIR/train.src\n",
    "!python space.py --infile $DIR/train.tgt --outfile $DIR/train.tgt\n",
    "!python space.py --infile $DIR/valid.src --outfile $DIR/valid.src\n",
    "!python space.py --infile $DIR/valid.tgt --outfile $DIR/valid.tgt\n",
    "!python space.py --infile $DIR/test.src --outfile $DIR/test.src\n",
    "!python space.py --infile $DIR/test.tgt --outfile $DIR/test.tgt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsonl形式に変換する\n",
    "\n",
    "!python format.py --src $DIR/train.src --tgt $DIR/train.tgt --outfile $DIR/train_$ver.jsonl --src_key kanji --tgt_key kana\n",
    "!python format.py --src $DIR/test.src --tgt $DIR/test.tgt --outfile $DIR/test_$ver.jsonl --src_key kanji --tgt_key kana\n",
    "!python format.py --src $DIR/valid.src --tgt $DIR/valid.tgt --outfile $DIR/valid_$ver.jsonl --src_key kanji --tgt_key kana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 漢字・アルファベットとカタカナのペアから学習する\n",
    "学習は，訓練用データを用いて学習し，１エポックごとに開発用データ(valid.json)でLossを確認する。\n",
    "\n",
    "\n",
    "### パラメタ\n",
    "- emb_size \n",
    "入力，出力の文字のエンベッドのDimension\n",
    "\n",
    "- nhead    \n",
    "マルチヘッド数\n",
    "\n",
    "- ffn_hid_dim    \n",
    "FFNのDimension\n",
    "\n",
    "- num_encoder_layer    \n",
    "エンコードレイヤーの数\n",
    "\n",
    "- num_decoder_layer    \n",
    "デコードレイヤーの数\n",
    "\n",
    "- lr     \n",
    "学習率\n",
    "\n",
    "- dropout    \n",
    "ドロップアウトの割合, 0-1\n",
    "\n",
    "- num_epochs    \n",
    "何周学習データを用いて学習を行うか\n",
    "\n",
    "- device    \n",
    "mps,cpu,cudaから選択\n",
    "  - mps        \n",
    "   アップルシリコンを搭載したマシンで実行する際に選択\n",
    "  - cuda         \n",
    "   CUDAが利用できる環境で選択\n",
    "  - cpu        \n",
    "   上記以外は，CPUモードを選択\n",
    "\n",
    "- earlystop_patient    \n",
    "開発用データでlossが下がらなくなってearlystop_patient回計算が終了した場合に，num_epochs数以下でも，計算を終了させる\n",
    "\n",
    "- output_dir    \n",
    "モデルの出力ディレクトリ\n",
    "\n",
    "- tensorboard_logdir    \n",
    "tensorboard形式のログの出力ディレクトリ。学習状況を確認するためには`tensorboard --logdir logs`を実行後，ブラウザでhttp://localhost:6000/から確認\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "- prefix    \n",
    "jsonl形式のデータのprefix\n",
    "\n",
    "- source_lang    \n",
    "jsonl形式のデータのsourceのキー\n",
    "\n",
    "- target_lang    \n",
    "jsonl形式のデータのtargetのキー\n",
    "\n",
    "- train_file    \n",
    "訓練用データのJSONLファイル\n",
    "\n",
    "- valid_file    \n",
    "開発用データのJSONLファイル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epochs:152\n",
      "Epoch: 1, Train loss: 3.252, Val loss: 2.531, Epoch time = 7308.093s\n",
      "Epoch: 2, Train loss: 2.431, Val loss: 1.917, Epoch time = 7302.768s\n",
      "Epoch: 3, Train loss: 2.042, Val loss: 1.601, Epoch time = 7287.174s\n",
      "Epoch: 4, Train loss: 1.792, Val loss: 1.360, Epoch time = 7280.848s\n",
      "Epoch: 5, Train loss: 1.584, Val loss: 1.167, Epoch time = 7290.536s\n",
      "Epoch: 6, Train loss: 1.423, Val loss: 1.030, Epoch time = 7310.489s\n",
      "Epoch: 7, Train loss: 1.300, Val loss: 0.926, Epoch time = 7278.405s\n",
      "Epoch: 8, Train loss: 1.205, Val loss: 0.848, Epoch time = 7257.939s\n",
      "Epoch: 9, Train loss: 1.127, Val loss: 0.785, Epoch time = 7301.632s\n",
      "Epoch: 10, Train loss: 1.060, Val loss: 0.728, Epoch time = 7261.019s\n",
      "Epoch: 11, Train loss: 1.002, Val loss: 0.685, Epoch time = 7275.680s\n",
      "Epoch: 12, Train loss: 0.950, Val loss: 0.648, Epoch time = 7260.727s\n",
      "Epoch: 13, Train loss: 0.904, Val loss: 0.607, Epoch time = 7261.540s\n",
      "Epoch: 14, Train loss: 0.863, Val loss: 0.575, Epoch time = 7332.567s\n",
      "Epoch: 15, Train loss: 0.826, Val loss: 0.549, Epoch time = 7265.682s\n",
      "Epoch: 16, Train loss: 0.793, Val loss: 0.528, Epoch time = 7263.989s\n",
      "Epoch: 17, Train loss: 0.764, Val loss: 0.504, Epoch time = 7262.051s\n",
      "Epoch: 18, Train loss: 0.737, Val loss: 0.484, Epoch time = 7335.368s\n",
      "Epoch: 19, Train loss: 0.713, Val loss: 0.467, Epoch time = 7259.125s\n",
      "Epoch: 20, Train loss: 0.691, Val loss: 0.448, Epoch time = 7306.743s\n",
      "Epoch: 21, Train loss: 0.670, Val loss: 0.436, Epoch time = 7334.791s\n",
      "Epoch: 22, Train loss: 0.651, Val loss: 0.425, Epoch time = 7272.576s\n",
      "Epoch: 23, Train loss: 0.634, Val loss: 0.414, Epoch time = 7343.253s\n",
      "Epoch: 24, Train loss: 0.618, Val loss: 0.402, Epoch time = 7333.673s\n",
      "Epoch: 25, Train loss: 0.603, Val loss: 0.389, Epoch time = 7337.730s\n",
      "Epoch: 26, Train loss: 0.588, Val loss: 0.381, Epoch time = 7258.300s\n",
      "Epoch: 27, Train loss: 0.575, Val loss: 0.374, Epoch time = 7254.173s\n",
      "Epoch: 28, Train loss: 0.562, Val loss: 0.364, Epoch time = 7264.911s\n",
      "Epoch: 29, Train loss: 0.550, Val loss: 0.354, Epoch time = 7258.657s\n",
      "Epoch: 30, Train loss: 0.539, Val loss: 0.348, Epoch time = 7253.059s\n",
      "Epoch: 31, Train loss: 0.528, Val loss: 0.340, Epoch time = 7327.717s\n",
      "Epoch: 32, Train loss: 0.518, Val loss: 0.337, Epoch time = 7258.898s\n",
      "Epoch: 33, Train loss: 0.508, Val loss: 0.328, Epoch time = 7263.710s\n",
      "Epoch: 34, Train loss: 0.499, Val loss: 0.323, Epoch time = 7340.335s\n",
      "Epoch: 35, Train loss: 0.491, Val loss: 0.320, Epoch time = 7291.350s\n",
      "Epoch: 36, Train loss: 0.482, Val loss: 0.313, Epoch time = 7257.607s\n",
      "Epoch: 37, Train loss: 0.474, Val loss: 0.308, Epoch time = 7260.522s\n",
      "Epoch: 38, Train loss: 0.466, Val loss: 0.304, Epoch time = 7251.455s\n",
      "Epoch: 39, Train loss: 0.459, Val loss: 0.299, Epoch time = 7261.361s\n",
      "Epoch: 40, Train loss: 0.452, Val loss: 0.294, Epoch time = 7264.532s\n",
      "Epoch: 41, Train loss: 0.445, Val loss: 0.290, Epoch time = 7322.228s\n",
      "Epoch: 42, Train loss: 0.439, Val loss: 0.287, Epoch time = 7258.952s\n",
      "Epoch: 43, Train loss: 0.433, Val loss: 0.281, Epoch time = 7256.994s\n",
      "Epoch: 44, Train loss: 0.427, Val loss: 0.279, Epoch time = 7256.875s\n",
      "Epoch: 45, Train loss: 0.421, Val loss: 0.277, Epoch time = 7261.547s\n",
      "Epoch: 46, Train loss: 0.416, Val loss: 0.270, Epoch time = 7241.594s\n",
      "Epoch: 47, Train loss: 0.410, Val loss: 0.270, Epoch time = 7249.422s\n",
      "Epoch: 48, Train loss: 0.405, Val loss: 0.267, Epoch time = 7323.113s\n",
      "Epoch: 49, Train loss: 0.400, Val loss: 0.262, Epoch time = 7249.335s\n",
      "Epoch: 50, Train loss: 0.395, Val loss: 0.260, Epoch time = 7250.444s\n",
      "Epoch: 51, Train loss: 0.390, Val loss: 0.259, Epoch time = 7251.241s\n",
      "Epoch: 52, Train loss: 0.386, Val loss: 0.254, Epoch time = 7252.399s\n",
      "Epoch: 53, Train loss: 0.381, Val loss: 0.252, Epoch time = 7239.971s\n",
      "Epoch: 54, Train loss: 0.377, Val loss: 0.251, Epoch time = 7243.280s\n",
      "Epoch: 55, Train loss: 0.373, Val loss: 0.246, Epoch time = 7316.028s\n",
      "Epoch: 56, Train loss: 0.369, Val loss: 0.246, Epoch time = 7325.163s\n",
      "Epoch: 57, Train loss: 0.365, Val loss: 0.242, Epoch time = 7273.375s\n",
      "Epoch: 58, Train loss: 0.362, Val loss: 0.240, Epoch time = 7259.251s\n",
      "Epoch: 59, Train loss: 0.358, Val loss: 0.237, Epoch time = 7256.532s\n",
      "Epoch: 60, Train loss: 0.354, Val loss: 0.235, Epoch time = 7253.403s\n",
      "Epoch: 61, Train loss: 0.350, Val loss: 0.234, Epoch time = 7325.651s\n",
      "Epoch: 62, Train loss: 0.346, Val loss: 0.231, Epoch time = 7324.665s\n",
      "Epoch: 63, Train loss: 0.342, Val loss: 0.227, Epoch time = 7256.549s\n",
      "Epoch: 64, Train loss: 0.336, Val loss: 0.222, Epoch time = 7254.187s\n",
      "Epoch: 65, Train loss: 0.330, Val loss: 0.222, Epoch time = 7248.369s\n",
      "Epoch: 66, Train loss: 0.325, Val loss: 0.218, Epoch time = 7265.407s\n",
      "Epoch: 67, Train loss: 0.320, Val loss: 0.213, Epoch time = 7329.041s\n",
      "Epoch: 68, Train loss: 0.315, Val loss: 0.212, Epoch time = 7256.920s\n",
      "Epoch: 69, Train loss: 0.311, Val loss: 0.210, Epoch time = 7332.148s\n",
      "Epoch: 70, Train loss: 0.306, Val loss: 0.210, Epoch time = 7244.675s\n",
      "Epoch: 71, Train loss: 0.303, Val loss: 0.207, Epoch time = 7241.800s\n",
      "Epoch: 72, Train loss: 0.299, Val loss: 0.203, Epoch time = 7249.465s\n",
      "Epoch: 73, Train loss: 0.296, Val loss: 0.201, Epoch time = 7290.621s\n",
      "Epoch: 74, Train loss: 0.293, Val loss: 0.200, Epoch time = 7248.802s\n",
      "Epoch: 75, Train loss: 0.289, Val loss: 0.196, Epoch time = 7244.515s\n",
      "Epoch: 76, Train loss: 0.286, Val loss: 0.196, Epoch time = 7250.976s\n",
      "Epoch: 77, Train loss: 0.282, Val loss: 0.192, Epoch time = 7245.164s\n",
      "Epoch: 78, Train loss: 0.279, Val loss: 0.193, Epoch time = 7318.035s\n",
      "Epoch: 79, Train loss: 0.275, Val loss: 0.191, Epoch time = 7239.420s\n",
      "Epoch: 80, Train loss: 0.272, Val loss: 0.188, Epoch time = 7254.989s\n",
      "Epoch: 81, Train loss: 0.268, Val loss: 0.186, Epoch time = 7256.297s\n",
      "Epoch: 82, Train loss: 0.265, Val loss: 0.186, Epoch time = 7241.785s\n",
      "Epoch: 83, Train loss: 0.262, Val loss: 0.182, Epoch time = 7244.317s\n",
      "Epoch: 84, Train loss: 0.259, Val loss: 0.181, Epoch time = 7243.853s\n",
      "Epoch: 85, Train loss: 0.257, Val loss: 0.180, Epoch time = 7314.401s\n",
      "Epoch: 86, Train loss: 0.254, Val loss: 0.179, Epoch time = 7264.275s\n",
      "Epoch: 87, Train loss: 0.252, Val loss: 0.180, Epoch time = 7255.615s\n",
      "Epoch: 88, Train loss: 0.249, Val loss: 0.177, Epoch time = 7241.195s\n",
      "Epoch: 89, Train loss: 0.247, Val loss: 0.174, Epoch time = 7238.254s\n",
      "Epoch: 90, Train loss: 0.245, Val loss: 0.174, Epoch time = 7239.503s\n",
      "Epoch: 91, Train loss: 0.243, Val loss: 0.173, Epoch time = 7249.744s\n",
      "Epoch: 92, Train loss: 0.241, Val loss: 0.172, Epoch time = 7275.367s\n",
      "Epoch: 93, Train loss: 0.238, Val loss: 0.171, Epoch time = 7253.639s\n",
      "Epoch: 94, Train loss: 0.237, Val loss: 0.171, Epoch time = 7305.959s\n",
      "Epoch: 95, Train loss: 0.234, Val loss: 0.167, Epoch time = 7258.156s\n",
      "Epoch: 96, Train loss: 0.233, Val loss: 0.170, Epoch time = 7257.112s\n",
      "Epoch: 97, Train loss: 0.231, Val loss: 0.168, Epoch time = 7250.658s\n",
      "Epoch: 98, Train loss: 0.229, Val loss: 0.166, Epoch time = 7243.141s\n",
      "Epoch: 99, Train loss: 0.228, Val loss: 0.165, Epoch time = 7312.912s\n",
      "Epoch: 100, Train loss: 0.226, Val loss: 0.165, Epoch time = 7242.625s\n",
      "Epoch: 101, Train loss: 0.224, Val loss: 0.165, Epoch time = 7245.832s\n",
      "Epoch: 102, Train loss: 0.223, Val loss: 0.162, Epoch time = 7248.791s\n",
      "Epoch: 103, Train loss: 0.221, Val loss: 0.161, Epoch time = 7255.897s\n",
      "Epoch: 104, Train loss: 0.219, Val loss: 0.161, Epoch time = 7254.507s\n",
      "Epoch: 105, Train loss: 0.218, Val loss: 0.161, Epoch time = 7309.587s\n",
      "Epoch: 106, Train loss: 0.217, Val loss: 0.158, Epoch time = 7331.000s\n",
      "Epoch: 107, Train loss: 0.215, Val loss: 0.160, Epoch time = 7297.224s\n",
      "Epoch: 108, Train loss: 0.214, Val loss: 0.158, Epoch time = 7260.923s\n",
      "Epoch: 109, Train loss: 0.213, Val loss: 0.156, Epoch time = 7248.466s\n",
      "Epoch: 110, Train loss: 0.211, Val loss: 0.156, Epoch time = 7236.382s\n",
      "Epoch: 111, Train loss: 0.210, Val loss: 0.156, Epoch time = 7311.518s\n",
      "Epoch: 112, Train loss: 0.209, Val loss: 0.154, Epoch time = 7247.321s\n",
      "Epoch: 113, Train loss: 0.208, Val loss: 0.155, Epoch time = 7244.123s\n",
      "Epoch: 114, Train loss: 0.206, Val loss: 0.155, Epoch time = 7252.162s\n",
      "Epoch: 115, Train loss: 0.205, Val loss: 0.155, Epoch time = 7245.962s\n",
      "Epoch: 116, Train loss: 0.204, Val loss: 0.153, Epoch time = 7252.693s\n",
      "Epoch: 117, Train loss: 0.203, Val loss: 0.153, Epoch time = 7254.212s\n",
      "Epoch: 118, Train loss: 0.202, Val loss: 0.152, Epoch time = 7281.502s\n",
      "Epoch: 119, Train loss: 0.201, Val loss: 0.151, Epoch time = 7285.826s\n",
      "Epoch: 120, Train loss: 0.200, Val loss: 0.151, Epoch time = 7263.496s\n",
      "Epoch: 121, Train loss: 0.198, Val loss: 0.151, Epoch time = 7235.547s\n",
      "Epoch: 122, Train loss: 0.198, Val loss: 0.149, Epoch time = 7243.732s\n",
      "Epoch: 123, Train loss: 0.197, Val loss: 0.149, Epoch time = 7242.334s\n",
      "Epoch: 124, Train loss: 0.195, Val loss: 0.148, Epoch time = 7248.045s\n",
      "Epoch: 125, Train loss: 0.194, Val loss: 0.147, Epoch time = 7242.069s\n",
      "Epoch: 126, Train loss: 0.194, Val loss: 0.147, Epoch time = 7244.548s\n",
      "Epoch: 127, Train loss: 0.193, Val loss: 0.149, Epoch time = 7254.163s\n",
      "Epoch: 128, Train loss: 0.192, Val loss: 0.146, Epoch time = 7250.529s\n",
      "Epoch: 129, Train loss: 0.191, Val loss: 0.146, Epoch time = 7310.842s\n",
      "Epoch: 130, Train loss: 0.190, Val loss: 0.147, Epoch time = 7234.830s\n",
      "Epoch: 131, Train loss: 0.189, Val loss: 0.146, Epoch time = 7309.563s\n",
      "Epoch: 132, Train loss: 0.188, Val loss: 0.146, Epoch time = 7322.747s\n",
      "Epoch: 133, Train loss: 0.187, Val loss: 0.143, Epoch time = 7245.954s\n",
      "Epoch: 134, Train loss: 0.186, Val loss: 0.144, Epoch time = 7281.495s\n",
      "Epoch: 135, Train loss: 0.185, Val loss: 0.145, Epoch time = 7238.013s\n",
      "Epoch: 136, Train loss: 0.185, Val loss: 0.144, Epoch time = 7307.045s\n",
      "Epoch: 137, Train loss: 0.184, Val loss: 0.143, Epoch time = 7275.361s\n"
     ]
    }
   ],
   "source": [
    "# 訓練実行\n",
    "\n",
    "!python ./char_model.py \\\n",
    "  --emb_size 512 \\\n",
    "  --nhead 8 \\\n",
    "  --ffn_hid_dim 2048 \\\n",
    "  --batch_size 128 \\\n",
    "  --num_encoder_layers 8 \\\n",
    "  --num_decoder_layers 8 \\\n",
    "  --lr 0.00002 \\\n",
    "  --dropout 0.3 \\\n",
    "  --num_epochs 152 \\\n",
    "  --device cuda \\\n",
    "  --earlystop_patient 3 \\\n",
    "  --output_dir $MODEL \\\n",
    "  --tensorboard_logdir $LOG \\\n",
    "  --prefix translation \\\n",
    "  --source_lang kanji \\\n",
    "  --target_lang kana \\\n",
    "  --train_file $DIR/train_$ver.jsonl \\\n",
    "  --valid_file $DIR/valid_$ver.jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs.$ver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルを検証する\n",
    "作成したモデルを，検証用データを用いて検証し，正解率を算出する\n",
    "\n",
    "\n",
    "## パラメタ\n",
    "\n",
    "- test_file\n",
    "jsonl形式の検証用データファイル\n",
    "\n",
    "- model_file\n",
    "transformer_model.pyで作成したモデルファイル。\n",
    "\n",
    "- outfile\n",
    "検証結果を格納するファイル\n",
    "\n",
    "- device\n",
    "cpu限定\n",
    "\n",
    "- nbest\n",
    "検証用データの漢字姓名を入力データとした時に，モデルが出力するカタカナ姓名を確率の高い方からnbest個出力する。beam_width以下。searchパラメタがbothかbeamの時有効\n",
    "\n",
    "- beam_width\n",
    "ビームサーチのビーム幅。searchパラメタがbothかbeamの時有効\n",
    "\n",
    "- max_len\n",
    "出力するカタカナ姓名の最大長さ\n",
    "\n",
    "- search\n",
    "greedy,beam,bothから選択\n",
    "-- greedy\n",
    "貪欲法による探索\n",
    "-- beam\n",
    "ビームサーチによる探索\n",
    "-- both\n",
    "貪欲砲，ビームサーチ両方の探索\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検証用データを実行\n",
    "\n",
    "\n",
    "!python generate_batch.py \\\n",
    "  --test_file $DIR/test.jsonl \\\n",
    "  --model_file $MODEL/checkpoint_best.pt \\\n",
    "  --outfile $DIR/generate.txt \\\n",
    "  --device cpu \\\n",
    "  --nbest 5 \\\n",
    "  --beam_width 5 \\\n",
    "  --max_len 100 \\\n",
    "  --search greedy\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc=0.7621229004168214\n"
     ]
    }
   ],
   "source": [
    "# 検証結果を評価\n",
    "import pandas as pd\n",
    "import os\n",
    "dr=os.environ[\"DIR\"]\n",
    "df = pd.read_csv(f'{dr}/generate.txt',encoding='utf-8',sep=\"\\t\")\n",
    "tgt = df['tgt'].tolist()\n",
    "pred = df['pred'].tolist()\n",
    "ok=0\n",
    "for t,p in zip(tgt,pred):\n",
    "    if t == p:\n",
    "        ok+=1\n",
    "\n",
    "print(f'acc={ok/len(tgt)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルをJavaのDJL用に変換する\n",
    "Pytorchで学習したモデルをJavaで使えるようにモデルの変換を行う。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 漢字からカナ\n",
    "!python convert_jitscript.py  \\\n",
    "    --model_file=$MODEL/checkpoint_best.pt \\\n",
    "    --model_script=$MODEL/script.pt \\\n",
    "    --encoder=$MODEL/encoder.pt \\\n",
    "    --decoder=$MODEL/decoder.pt \\\n",
    "    --positional_encoding=$MODEL/positional_encoding.pt \\\n",
    "    --generator=$MODEL/generator.pt \\\n",
    "    --src_tok_emb=$MODEL/src_tok_emb.pt \\\n",
    "    --tgt_tok_emb=$MODEL/tgt_tok_emb.pt \\\n",
    "    --vocab_src=$MODEL/vocab_src.txt \\\n",
    "    --vocab_tgt=$MODEL/vocab_tgt.txt \\\n",
    "    --params=$MODEL/params.json \\\n",
    "    --device=cpu\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
