{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 漢字・アルファベットからカタカナを推測するモデルを学習する\n",
    "オープンソース辞書及び，Wikipediaから作成した漢字とカタカナのペアデータを用いて，Transformer+Seq2Seqのモデルで，スクラッチで学習する。\n",
    "\n",
    "入力の文字（漢字・アルファベットなど）は一文字ずつに分割，また，出力の文字（カタカナ）も一文字ずつに分割して，学習する。\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ準備\n",
    "データは，[dict](../dict)で作成した姓名辞書データのoss.json及びseimei.jsonと，Wikipediaから抜き出した，漢字姓名とカタカナ姓名（スペース区切りのもの）を用いて，学習に使用した。単語単位である姓名辞書データと，Wikipediaのスペースで区切られた姓名を用いることで，姓名で漢字とカナが入力された場合の学習を可能とした。\n",
    "\n",
    "また，ひらがなとカタカナ，\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スクリプトで使う環境変数をセット\n",
    "import os\n",
    "dict_ver='1.6o'\n",
    "model_ver='1.6.1o'\n",
    "\n",
    "os.environ['dict_ver']=dict_ver\n",
    "os.environ['model_ver']=model_ver\n",
    "\n",
    "os.environ['DIR']=f'dataset.{model_ver}'\n",
    "os.environ['MODEL']=f'model.{model_ver}'\n",
    "os.environ[\"LOG\"]=f'logs.{model_ver}'\n",
    "os.environ['RATIO']=\"0.01\"\n",
    "os.environ['WIKI']=\"../dict/wikipedia/wikiname.txt\" # Wikipediaの概要欄から取得した姓名の漢字・アルファベットとカタカナのペアデータ。このファイルの姓名をスペースで区切って丹後他院位にしたものは，辞書ファイルに追加済み\n",
    "\n",
    "os.environ[\"DICT\"]=f'../dict/data/data_{dict_ver}'\n",
    "\n",
    "os.makedirs(os.environ['DIR'],exist_ok=True)\n",
    "os.makedirs(os.environ['MODEL'],exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"dic1\": \"../dict/data/data_1.6o/oss_1.6o.json\",\n",
      "  \"dic2\": \"../dict/data/data_1.6o/seimei_1.6o.json\",\n",
      "  \"outfile\": \"tmp.json\"\n",
      "}\n",
      "len=2046560\n",
      "cnt=2161305\n"
     ]
    }
   ],
   "source": [
    "# オープンソース辞書とWikipediaから作成した単語辞書をマージする\n",
    "!python mergedic.py --dic1 $DICT/oss_$dict_ver.json --dic2 $DICT/seimei_$dict_ver.json --outfile tmp.json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"json\": \"tmp.json\",\n",
      "  \"outdir\": \"dataset.1.6.1o\",\n",
      "  \"ratio\": 0.01,\n",
      "  \"reverse\": false\n",
      "}\n",
      "k_max_len=72\n",
      "v_max_len=120\n",
      "train,2118079\n",
      "valid,21613\n",
      "test,21613\n"
     ]
    }
   ],
   "source": [
    "# 作成したデータを訓練用，開発用，検証用に分ける。開発用と検証用がそれぞれ全体の0.01\n",
    "!python prep.py --json tmp.json --outdir $DIR --ratio $RATIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikipediaの漢字姓名とカナ姓名を取得し，訓練，開発，検証用に分けて，辞書から作成したデータに追加する。\n",
    "!python catawk.py --begin_idx_per 0 --end_idx_per 1 --appendfile $DIR/valid.src --index 0 --infile $WIKI --infile_delimiter tsv\n",
    "!python catawk.py --begin_idx_per 0 --end_idx_per 1 --appendfile $DIR/valid.tgt --index 1 --infile $WIKI --infile_delimiter tsv\n",
    "!python catawk.py --begin_idx_per 1 --end_idx_per 2 --appendfile $DIR/test.src --index 0 --infile $WIKI --infile_delimiter tsv\n",
    "!python catawk.py --begin_idx_per 1 --end_idx_per 2 --appendfile $DIR/test.tgt --index 1 --infile $WIKI --infile_delimiter tsv\n",
    "!python catawk.py --begin_idx_per 2 --end_idx_per 100 --appendfile $DIR/train.src --index 0 --infile $WIKI --infile_delimiter tsv\n",
    "!python catawk.py --begin_idx_per 2 --end_idx_per 100 --appendfile $DIR/train.tgt --index 1 --infile $WIKI --infile_delimiter tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"outfile\": \"dataset.1.6.1o/hirakata.txt\",\n",
      "  \"reverse\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ひらがなとカタカナのペアデータを作成し，訓練データに混ぜる\n",
    "!python kana.py --outfile $DIR/hirakata.txt\n",
    "\n",
    "!python catawk.py --begin_idx_per 0 --end_idx_per 100 --appendfile $DIR/train.src --index 0 --infile $DIR/hirakata.txt\n",
    "!python catawk.py --begin_idx_per 0 --end_idx_per 100 --appendfile $DIR/train.tgt --index 1 --infile $DIR/hirakata.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"jsonfile\": \"../dict/data/data_1.6o/tankanji_1.6o.json\",\n",
      "  \"outfile\": \"tankanji.txt\",\n",
      "  \"reverse\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 単漢字とカタカナのデータを，訓練データに混ぜる \n",
    "!python jsontotext.py --jsonfile $DICT/tankanji_$dict_ver.json --outfile tankanji.txt\n",
    "\n",
    "!python catawk.py --begin_idx_per 0 --end_idx_per 100 --appendfile $DIR/train.src --index 0 --infile tankanji.txt\n",
    "!python catawk.py --begin_idx_per 0 --end_idx_per 100 --appendfile $DIR/train.tgt --index 1 --infile tankanji.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"train_src\": \"dataset.1.6.1o/train.src\",\n",
      "  \"train_tgt\": \"dataset.1.6.1o/train.tgt\",\n",
      "  \"test_src\": \"dataset.1.6.1o/test.src\",\n",
      "  \"test_tgt\": \"dataset.1.6.1o/test.tgt\",\n",
      "  \"valid_src\": \"dataset.1.6.1o/valid.src\",\n",
      "  \"valid_tgt\": \"dataset.1.6.1o/valid.tgt\"\n",
      "}\n",
      "train=2313234\n",
      "omit=540\n"
     ]
    }
   ],
   "source": [
    "# 開発，検証用に，訓練データに入っているものがあれば，削除する(インサンプルを阻止)\n",
    "!python omit_testval.py --train_src $DIR/train.src --train_tgt $DIR/train.tgt --test_src $DIR/test.src --test_tgt $DIR/test.tgt --valid_src $DIR/valid.src --valid_tgt $DIR/valid.tgt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"tgt\": \"dataset.1.6.1o/train.tgt\",\n",
      "  \"src\": \"dataset.1.6.1o/train.src\"\n",
      "}\n",
      "{\n",
      "  \"tgt\": \"dataset.1.6.1o/valid.tgt\",\n",
      "  \"src\": \"dataset.1.6.1o/valid.src\"\n",
      "}\n",
      "{\n",
      "  \"tgt\": \"dataset.1.6.1o/test.tgt\",\n",
      "  \"src\": \"dataset.1.6.1o/test.src\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 訓練，開発，検証データをシャッフルする\n",
    "\n",
    "!python shuffle.py --src $DIR/train.src --tgt $DIR/train.tgt\n",
    "!python shuffle.py --src $DIR/valid.src --tgt $DIR/valid.tgt\n",
    "!python shuffle.py --src $DIR/test.src --tgt $DIR/test.tgt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025年  6月  6日 金曜日 19:07:30 JST\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スクリプトで使う環境変数をセット\n",
    "import os\n",
    "os.environ['VER']='1.7o'\n",
    "os.environ['DIR']='dataset/dataset.'+os.environ['VER']\n",
    "os.environ['MODEL']='model/model.'+os.environ['VER']\n",
    "os.environ[\"LOG\"]='logs/logs.'+os.environ['VER']\n",
    "os.environ['RATIO']=\"0.01\"\n",
    "os.environ['WIKI']=\"../dict/oss/wikipedia/wikiname.txt\" # Wikipediaの概要欄から取得した姓名の漢字・アルファベットとカタカナのペアデータ。このファイルの姓名をスペースで区切って丹後他院位にしたものは，辞書ファイルに追加済み\n",
    "os.environ['DICT']='../dict/data/data.'+os.environ['VER']\n",
    "os.makedirs(os.environ['DIR'],exist_ok=True)\n",
    "os.makedirs(os.environ['MODEL'],exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 漢字・アルファベットとカタカナのペアから学習する\n",
    "学習は，訓練用データを用いて学習し，１エポックごとに開発用データ(valid.json)でLossを確認する。\n",
    "\n",
    "\n",
    "### パラメタ\n",
    "- emb_size \n",
    "入力，出力の文字のエンベッドのDimension\n",
    "\n",
    "- nhead    \n",
    "マルチヘッド数\n",
    "\n",
    "- ffn_hid_dim    \n",
    "FFNのDimension\n",
    "\n",
    "- num_encoder_layer    \n",
    "エンコードレイヤーの数\n",
    "\n",
    "- num_decoder_layer    \n",
    "デコードレイヤーの数\n",
    "\n",
    "- lr     \n",
    "学習率\n",
    "\n",
    "- dropout    \n",
    "ドロップアウトの割合, 0-1\n",
    "\n",
    "- num_epochs    \n",
    "何周学習データを用いて学習を行うか\n",
    "\n",
    "- device    \n",
    "mps,cpu,cudaから選択\n",
    "  - mps        \n",
    "   アップルシリコンを搭載したマシンで実行する際に選択\n",
    "  - cuda         \n",
    "   CUDAが利用できる環境で選択\n",
    "  - cpu        \n",
    "   上記以外は，CPUモードを選択\n",
    "\n",
    "- earlystop_patient    \n",
    "開発用データでlossが下がらなくなってearlystop_patient回計算が終了した場合に，num_epochs数以下でも，計算を終了させる\n",
    "\n",
    "- output_dir    \n",
    "モデルの出力ディレクトリ\n",
    "\n",
    "- tensorboard_logdir    \n",
    "tensorboard形式のログの出力ディレクトリ。学習状況を確認するためには`tensorboard --logdir logs`を実行後，ブラウザでhttp://localhost:6000/から確認\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "- prefix    \n",
    "jsonl形式のデータのprefix\n",
    "\n",
    "- source_lang    \n",
    "jsonl形式のデータのsourceのキー\n",
    "\n",
    "- target_lang    \n",
    "jsonl形式のデータのtargetのキー\n",
    "\n",
    "- train_file    \n",
    "訓練用データのJSONLファイル\n",
    "\n",
    "- valid_file    \n",
    "開発用データのJSONLファイル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epochs:152\n",
      "Epoch: 1, Train loss: 3.283, Val loss: 2.572, Epoch time = 6332.798s\n",
      "Epoch: 2, Train loss: 2.475, Val loss: 1.982, Epoch time = 6339.903s\n",
      "Epoch: 3, Train loss: 2.094, Val loss: 1.663, Epoch time = 6328.976s\n",
      "Epoch: 4, Train loss: 1.837, Val loss: 1.416, Epoch time = 6305.212s\n",
      "Epoch: 5, Train loss: 1.631, Val loss: 1.236, Epoch time = 6295.878s\n",
      "Epoch: 6, Train loss: 1.471, Val loss: 1.105, Epoch time = 6288.100s\n",
      "Epoch: 7, Train loss: 1.347, Val loss: 1.001, Epoch time = 6284.781s\n",
      "Epoch: 8, Train loss: 1.248, Val loss: 0.915, Epoch time = 6294.494s\n",
      "Epoch: 9, Train loss: 1.167, Val loss: 0.851, Epoch time = 6317.882s\n",
      "Epoch: 10, Train loss: 1.096, Val loss: 0.794, Epoch time = 6339.182s\n",
      "Epoch: 11, Train loss: 1.035, Val loss: 0.739, Epoch time = 6339.323s\n",
      "Epoch: 12, Train loss: 0.980, Val loss: 0.699, Epoch time = 6344.732s\n",
      "Epoch: 13, Train loss: 0.932, Val loss: 0.665, Epoch time = 6305.883s\n",
      "Epoch: 14, Train loss: 0.888, Val loss: 0.627, Epoch time = 6290.475s\n",
      "Epoch: 15, Train loss: 0.849, Val loss: 0.595, Epoch time = 6279.850s\n",
      "Epoch: 16, Train loss: 0.814, Val loss: 0.570, Epoch time = 6278.103s\n",
      "Epoch: 17, Train loss: 0.781, Val loss: 0.549, Epoch time = 6276.395s\n",
      "Epoch: 18, Train loss: 0.752, Val loss: 0.530, Epoch time = 6279.913s\n",
      "Epoch: 19, Train loss: 0.726, Val loss: 0.511, Epoch time = 6359.372s\n",
      "Epoch: 20, Train loss: 0.702, Val loss: 0.489, Epoch time = 6297.150s\n",
      "Epoch: 21, Train loss: 0.680, Val loss: 0.479, Epoch time = 6300.833s\n",
      "Epoch: 22, Train loss: 0.660, Val loss: 0.460, Epoch time = 6302.001s\n",
      "Epoch: 23, Train loss: 0.641, Val loss: 0.450, Epoch time = 6304.648s\n",
      "Epoch: 24, Train loss: 0.624, Val loss: 0.433, Epoch time = 6314.948s\n",
      "Epoch: 25, Train loss: 0.607, Val loss: 0.425, Epoch time = 6352.106s\n",
      "Epoch: 26, Train loss: 0.592, Val loss: 0.414, Epoch time = 6340.306s\n",
      "Epoch: 27, Train loss: 0.578, Val loss: 0.407, Epoch time = 6308.890s\n",
      "Epoch: 28, Train loss: 0.564, Val loss: 0.397, Epoch time = 6281.946s\n",
      "Epoch: 29, Train loss: 0.552, Val loss: 0.385, Epoch time = 6269.374s\n",
      "Epoch: 30, Train loss: 0.540, Val loss: 0.378, Epoch time = 6291.280s\n",
      "Epoch: 31, Train loss: 0.528, Val loss: 0.371, Epoch time = 6293.519s\n",
      "Epoch: 32, Train loss: 0.517, Val loss: 0.366, Epoch time = 6286.154s\n",
      "Epoch: 33, Train loss: 0.507, Val loss: 0.357, Epoch time = 6285.336s\n",
      "Epoch: 34, Train loss: 0.498, Val loss: 0.351, Epoch time = 6285.066s\n",
      "Epoch: 35, Train loss: 0.488, Val loss: 0.346, Epoch time = 6280.687s\n",
      "Epoch: 36, Train loss: 0.479, Val loss: 0.342, Epoch time = 6274.693s\n",
      "Epoch: 37, Train loss: 0.471, Val loss: 0.339, Epoch time = 6280.846s\n",
      "Epoch: 38, Train loss: 0.463, Val loss: 0.333, Epoch time = 6285.873s\n",
      "Epoch: 39, Train loss: 0.455, Val loss: 0.329, Epoch time = 6288.301s\n",
      "Epoch: 40, Train loss: 0.448, Val loss: 0.326, Epoch time = 6281.272s\n",
      "Epoch: 41, Train loss: 0.441, Val loss: 0.323, Epoch time = 6284.633s\n",
      "Epoch: 42, Train loss: 0.434, Val loss: 0.324, Epoch time = 6274.555s\n",
      "Epoch: 43, Train loss: 0.428, Val loss: 0.315, Epoch time = 6274.885s\n",
      "Epoch: 44, Train loss: 0.421, Val loss: 0.314, Epoch time = 6270.148s\n",
      "Epoch: 45, Train loss: 0.415, Val loss: 0.309, Epoch time = 6281.972s\n",
      "Epoch: 46, Train loss: 0.410, Val loss: 0.307, Epoch time = 6286.393s\n",
      "Epoch: 47, Train loss: 0.404, Val loss: 0.305, Epoch time = 6285.344s\n",
      "Epoch: 48, Train loss: 0.399, Val loss: 0.304, Epoch time = 6284.791s\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/analysis01/src/kanjikana-model/train/./char_model.py\", line 582, in <module>\n",
      "    main()\n",
      "  File \"/home/analysis01/src/kanjikana-model/train/./char_model.py\", line 578, in main\n",
      "    KanjiKanaTransformer(args).train()\n",
      "  File \"/home/analysis01/src/kanjikana-model/train/./char_model.py\", line 523, in train\n",
      "    train_loss = self.train_epoch(train_iter, transformer, optimizer, loss_fn)\n",
      "  File \"/home/analysis01/src/kanjikana-model/train/./char_model.py\", line 304, in train_epoch\n",
      "    return losses / len(list(train_dataloader))\n",
      "  File \"/home/analysis01/src/kanjikana-model/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 701, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/analysis01/src/kanjikana-model/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 757, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/home/analysis01/src/kanjikana-model/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"/home/analysis01/src/kanjikana-model/train/./char_model.py\", line 273, in collate_fn\n",
      "    tgt_batch.append(self.text_transform[self.args.target_lang](tgt_sample.rstrip(\"\\n\")))\n",
      "  File \"/home/analysis01/src/kanjikana-model/train/./char_model.py\", line 257, in func\n",
      "    txt_input = transform(txt_input)\n",
      "  File \"/home/analysis01/src/kanjikana-model/train/./char_model.py\", line 264, in tensor_transform\n",
      "    torch.tensor(token_ids),\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# 訓練実行\n",
    "\n",
    "!python ./char_model.py \\\n",
    "  --emb_size 512 \\\n",
    "  --nhead 8 \\\n",
    "  --ffn_hid_dim 2048 \\\n",
    "  --batch_size 128 \\\n",
    "  --num_encoder_layers 8 \\\n",
    "  --num_decoder_layers 8 \\\n",
    "  --lr 0.00002 \\\n",
    "  --dropout 0.3 \\\n",
    "  --num_epochs 152 \\\n",
    "  --device cuda \\\n",
    "  --earlystop_patient 3 \\\n",
    "  --output_dir $MODEL \\\n",
    "  --tensorboard_logdir $LOG \\\n",
    "  --prefix translation \\\n",
    "  --source_lang kanji \\\n",
    "  --target_lang kana \\\n",
    "  --train_file $DIR/train.jsonl \\\n",
    "  --valid_file $DIR/valid.jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-2b5b7ca7f90d5d5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-2b5b7ca7f90d5d5\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6009;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/logs.$VER"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルを検証する\n",
    "作成したモデルを，検証用データを用いて検証し，正解率を算出する\n",
    "\n",
    "\n",
    "## パラメタ\n",
    "\n",
    "- test_file\n",
    "jsonl形式の検証用データファイル\n",
    "\n",
    "- model_file\n",
    "transformer_model.pyで作成したモデルファイル。\n",
    "\n",
    "- outfile\n",
    "検証結果を格納するファイル\n",
    "\n",
    "- device\n",
    "cpu限定\n",
    "\n",
    "- nbest\n",
    "検証用データの漢字姓名を入力データとした時に，モデルが出力するカタカナ姓名を確率の高い方からnbest個出力する。beam_width以下。searchパラメタがbothかbeamの時有効\n",
    "\n",
    "- beam_width\n",
    "ビームサーチのビーム幅。searchパラメタがbothかbeamの時有効\n",
    "\n",
    "- max_len\n",
    "出力するカタカナ姓名の最大長さ\n",
    "\n",
    "- search\n",
    "greedy,beam,bothから選択\n",
    "-- greedy\n",
    "貪欲法による探索\n",
    "-- beam\n",
    "ビームサーチによる探索\n",
    "-- both\n",
    "貪欲砲，ビームサーチ両方の探索\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検証用データを実行\n",
    "\n",
    "\n",
    "!python generate_batch.py \\\n",
    "  --test_file $DIR/test.jsonl \\\n",
    "  --model_file $MODEL/checkpoint_best.pt \\\n",
    "  --outfile $DIR/generate.txt \\\n",
    "  --device cpu \\\n",
    "  --nbest 5 \\\n",
    "  --beam_width 5 \\\n",
    "  --max_len 100 \\\n",
    "  --search greedy\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc=0.6069319871997115\n"
     ]
    }
   ],
   "source": [
    "# 検証結果を評価\n",
    "import pandas as pd\n",
    "import os\n",
    "dr=os.environ[\"DIR\"]\n",
    "df = pd.read_csv(f'{dr}/generate.txt',encoding='utf-8',sep=\"\\t\")\n",
    "tgt = df['tgt'].tolist()\n",
    "pred = df['pred'].tolist()\n",
    "ok=0\n",
    "for t,p in zip(tgt,pred):\n",
    "    if t == p:\n",
    "        ok+=1\n",
    "\n",
    "print(f'acc={ok/len(tgt)}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルをJavaのDJL用に変換する\n",
    "Pytorchで学習したモデルをJavaで使えるようにモデルの変換を行う。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 漢字からカナ\n",
    "!python convert_jitscript.py  \\\n",
    "    --model_file=$MODEL/checkpoint_best.pt \\\n",
    "    --model_script=$MODEL/script_$VER.pt \\\n",
    "    --encoder=$MODEL/encoder_$VER.pt \\\n",
    "    --decoder=$MODEL/decoder_$VER.pt \\\n",
    "    --positional_encoding=$MODEL/positional_encoding_$VER.pt \\\n",
    "    --generator=$MODEL/generator_$VER.pt \\\n",
    "    --src_tok_emb=$MODEL/src_tok_emb_$VER.pt \\\n",
    "    --tgt_tok_emb=$MODEL/tgt_tok_emb_$VER.pt \\\n",
    "    --vocab_src=$MODEL/vocab_src_$VER.txt \\\n",
    "    --vocab_tgt=$MODEL/vocab_tgt_$VER.txt \\\n",
    "    --params=$MODEL/params_$VER.json \\\n",
    "    --device=cpu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025年  6月 10日 火曜日 11:41:45 JST\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# カタカナから漢字・アルファベットを推測するモデルを学習する\n",
    "オープンソース辞書及び，Wikipediaから作成した漢字とカタカナのペアデータを用いて，Transformerで，スクラッチで学習する。\n",
    "\n",
    "入力の文字（カタカナ）は一文字ずつに分割，また，出力の文字（漢字・アルファベットなど）も一文字ずつに分割して，学習する。\n",
    "\n",
    "データセットは，漢字からカタカナを推測するモデルのデータを，漢字とカナを逆にした者を作成して利用する。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ準備\n",
    "データは，[dict](../dict)で作成した姓名辞書データのoss.json及びseimei.jsonと，Wikipediaから抜き出した，漢字姓名とカタカナ姓名（スペース区切りのもの）を用いて，学習に使用した。単語単位である姓名辞書データと，Wikipediaのスペースで区切られた姓名を用いることで，姓名で漢字とカナが入力された場合の学習を可能とした。\n",
    "\n",
    "また，ひらがなとカタカナ，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スクリプトで使う環境変数をセット\n",
    "import os\n",
    "\n",
    "os.environ['DIR']='dataset/dataset.'+os.environ['VER']\n",
    "os.environ['MODEL']='model/model_r.'+os.environ['VER']\n",
    "os.environ[\"LOG\"]='logs/logs_r.'+os.environ['VER']\n",
    "os.environ['RATIO']=\"0.01\"\n",
    "\n",
    "os.makedirs(os.environ['DIR'],exist_ok=True)\n",
    "os.makedirs(os.environ['MODEL'],exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練実行\n",
    "\n",
    "!python ./char_model.py \\\n",
    "  --emb_size 512 \\\n",
    "  --nhead 8 \\\n",
    "  --ffn_hid_dim 2048 \\\n",
    "  --batch_size 32 \\\n",
    "  --num_encoder_layers 8 \\\n",
    "  --num_decoder_layers 8 \\\n",
    "  --lr 0.00002 \\\n",
    "  --dropout 0.3 \\\n",
    "  --num_epochs 145 \\\n",
    "  --device cuda  \\\n",
    "  --earlystop_patient 3 \\\n",
    "  --output_dir $MODEL \\\n",
    "  --tensorboard_logdir $LOG \\\n",
    "  --prefix translation \\\n",
    "  --source_lang kana \\\n",
    "  --target_lang kanji \\\n",
    "  --train_file $DIR/train.jsonl \\\n",
    "  --valid_file $DIR/valid.jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルを検証する\n",
    "作成したモデルを，検証用データを用いて検証し，正解率を算出する\n",
    "\n",
    "\n",
    "## パラメタ\n",
    "\n",
    "- test_file    \n",
    "jsonl形式の検証用データファイル\n",
    "\n",
    "- model_file    \n",
    "transformer_model.pyで作成したモデルファイル。\n",
    "\n",
    "- outfile    \n",
    "検証結果を格納するファイル\n",
    "\n",
    "- device    \n",
    "cpu限定\n",
    "\n",
    "- nbest    \n",
    "検証用データの漢字姓名を入力データとした時に，モデルが出力するカタカナ姓名を確率の高い方からnbest個出力する。beam_width以下。searchパラメタがbothかbeamの時有効\n",
    "\n",
    "- beam_width    \n",
    "ビームサーチのビーム幅。searchパラメタがbothかbeamの時有効\n",
    "\n",
    "- max_len    \n",
    "出力するカタカナ姓名の最大長さ\n",
    "\n",
    "- search    \n",
    "greedy,beam,bothから選択    \n",
    "  - greedy    \n",
    "貪欲法による探索    \n",
    "  - beam    \n",
    "ビームサーチによる探索    \n",
    "  - both    \n",
    "貪欲砲，ビームサーチ両方の探索    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検証用データを実行\n",
    "\n",
    "\n",
    "!python generate_batch.py \\\n",
    "  --test_file $DIR/test.jsonl \\\n",
    "  --model_file $MODEL/checkpoint_best.pt \\\n",
    "  --outfile $DIR/generate_r.txt \\\n",
    "  --device cpu \\\n",
    "  --nbest 5 \\\n",
    "  --beam_width 5 \\\n",
    "  --max_len 100 \\\n",
    "  --search greedy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検証結果を評価\n",
    "import pandas as pd\n",
    "import os\n",
    "dr=os.environ[\"DIR\"]\n",
    "df = pd.read_csv(f'{dr}/generate.txt',encoding='utf-8',sep=\"\\t\")\n",
    "tgt = df['tgt'].tolist()\n",
    "pred = df['pred'].tolist()\n",
    "ok=0\n",
    "for t,p in zip(tgt,pred):\n",
    "    if t == p:\n",
    "        ok+=1\n",
    "\n",
    "print(f'acc={ok/len(tgt)}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルをJavaのDJL用に変換する\n",
    "Pytorchで学習したモデルをJavaで使えるようにモデルの変換を行う。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カナから漢字\n",
    "!python convert_jitscript.py  \\\n",
    "    --model_file=$MODEL/checkpoint_best.pt \\\n",
    "    --model_script=$MODEL/script_$VER.pt \\\n",
    "    --encoder=$MODEL/encoder_$VER.pt \\\n",
    "    --decoder=$MODEL/decoder_$VER.pt \\\n",
    "    --positional_encoding=$MODEL/positional_encoding_$VER.pt \\\n",
    "    --generator=$MODEL/generator_$VER.pt \\\n",
    "    --src_tok_emb=$MODEL/src_tok_emb_$VER.pt \\\n",
    "    --tgt_tok_emb=$MODEL/tgt_tok_emb_$VER.pt \\\n",
    "    --vocab_src=$MODEL/vocab_src_$VER.txt \\\n",
    "    --vocab_tgt=$MODEL/vocab_tgt_$VER.txt \\\n",
    "    --params=$MODEL/params_$VER.json \\\n",
    "    --source_lang=kana \\\n",
    "    --target_lang=kanji \\\n",
    "    --device=cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!date"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
