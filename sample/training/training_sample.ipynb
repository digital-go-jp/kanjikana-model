{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIモデル訓練サンプル\n",
    "PythonでEng-Fraサンプルデータを用いて，FrenchからEnglishへ翻訳するモデルを訓練する。\n",
    "本モデルでは、単語単位ではなく、文字単位で分割し、学習します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## プログラム説明\n",
    "\n",
    "model.py\n",
    "\n",
    "内部で，pytorchのtransformerライブラリを呼び出している。各パラメタの詳細は [Pytorch](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html) を参照のこと。\n",
    "\n",
    "- emb_size    \n",
    "  単語のエンベッドサイズ\n",
    "- nhead    \n",
    "  transformerのMultiHeadAttentionのヘッド数\n",
    "- ffn_hid_dim       \n",
    "  FeedForwardNeuralNetworkの次元数\n",
    "- batch_size     \n",
    "  ミニバッチサイズ。メモリが足りないときや計算速度を早めたいときにはこのサイズを変更する\n",
    "- num_encoder_layers    \n",
    "  エンコーダ内のサブエンコーダ層の数 \n",
    "- num_decoder_layers    \n",
    "  デコーダ内のサブデコーダ層の数\n",
    "- lr   \n",
    "  学習率\n",
    "- dropout    \n",
    "  ドロップアウトの割合，1=100%\n",
    "- num_epochs    \n",
    "  学習用データを何周学習するか\n",
    "- device    \n",
    "  cuda: Cudaが使えるマシンではこれを選択\n",
    "  mps: Apple Silicornが使えるマシンではこれを選択\n",
    "  cpu: CPUで計算\n",
    "- earlystop_patient    \n",
    "  num_epochs以下でも，開発用データで，Lossが下がらなくなった回数がearlystop_patientより大きくなると，計算を終了させる\n",
    "- output_dir    \n",
    "  学習したモデルを格納するディレクトリ。ディレクトリには checkpoint_xxx.pt（xxxはepoch数）とcheckpoint_best.ptが作成され，valid lossが最も小さくなったepoch回のモデルをcheckpoint_best.ptとして保存する\n",
    "- tensorboard_logdir    \n",
    "  tensorboard のログを格納するディレクトリ。学習結果などを視覚化して表示できる。 tensorboard --logdir tensorboard_logdir で起動し，http://localhost:6006でアクセスすると表示される\n",
    "- prefix    \n",
    "  jsonl形式の訓練データ及び開発データのprefix\n",
    "- source_lang    \n",
    "  jsonl形式の訓練データ及び開発データでの，翻訳元となるデータにつけるキー\n",
    "- target_lang     \n",
    "  jsonl形式の訓練データ及び開発データでの，翻訳先となるデータにつけるキー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/analysis01/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/analysis01/nltk_data...\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/analysis01/src/kanjikana-model/sample/training/./model.py\", line 522, in <module>\n",
      "    main()\n",
      "  File \"/home/analysis01/src/kanjikana-model/sample/training/./model.py\", line 518, in main\n",
      "    KanjiKanaTransformer(args).train()\n",
      "  File \"/home/analysis01/src/kanjikana-model/sample/training/./model.py\", line 432, in train\n",
      "    train_iter = KanjiKanaDataSet(self.args, self.args.train_file, engfra_tokenizer(self.args.source_lang) , engfra_tokenizer(self.args.target_lang))\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/analysis01/src/kanjikana-model/sample/training/./model.py\", line 187, in __init__\n",
      "    with open(root,'r',encoding='utf-8') as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../dataset/train.jsonl'\n"
     ]
    }
   ],
   "source": [
    "# Pythonで訓練をする\n",
    "# deviceはcuda or cpu or mps\n",
    "#  mps: apple silicon\n",
    "# out of memoryが発生した際には、batch_sizeを減らす\n",
    "\n",
    "\n",
    "# 途中から計算するときには，modelディレクトリに checkpoint_xxx.pt(xxxは計算済みのepoch数)とcheckpoint_best.pt が存在すること\n",
    "\n",
    "!python ./model.py \\\n",
    "  --emb_size 1024 \\\n",
    "  --nhead 8 \\\n",
    "  --ffn_hid_dim 2048 \\\n",
    "  --batch_size 32 \\\n",
    "  --num_encoder_layers 12 \\\n",
    "  --num_decoder_layers 12 \\\n",
    "  --lr 0.00002 \\\n",
    "  --dropout 0.3 \\\n",
    "  --num_epochs 100 \\\n",
    "  --device cuda \\\n",
    "  --earlystop_patient 3 \\\n",
    "  --output_dir model \\\n",
    "  --tensorboard_logdir logs \\\n",
    "  --prefix translation \\\n",
    "  --source_lang fra \\\n",
    "  --target_lang eng \\\n",
    "  --train_file ../dataset/train.jsonl \\\n",
    "  --valid_file ../dataset/val.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# javaのDJLで使えるようにモデルファイルを変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/analysis01/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/analysis01/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/analysis01/src/kanjikana-model/sample/training/convert.py\", line 75, in <module>\n",
      "    main()\n",
      "  File \"/home/analysis01/src/kanjikana-model/sample/training/convert.py\", line 70, in main\n",
      "    KanjiKanaTransformerScripted(args).convert()\n",
      "  File \"/home/analysis01/src/kanjikana-model/sample/training/convert.py\", line 19, in convert\n",
      "    best_checkpoint = torch.load(self.args.model_file, map_location=torch.device(self.args.device))\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/analysis01/src/kanjikana-model/venv/lib/python3.11/site-packages/torch/serialization.py\", line 1319, in load\n",
      "    with _open_file_like(f, \"rb\") as opened_file:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/analysis01/src/kanjikana-model/venv/lib/python3.11/site-packages/torch/serialization.py\", line 659, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/analysis01/src/kanjikana-model/venv/lib/python3.11/site-packages/torch/serialization.py\", line 640, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "                     ^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'model/checkpoint_best.pt'\n"
     ]
    }
   ],
   "source": [
    "!python convert.py  \\\n",
    "    --model_file=model/checkpoint_best.pt \\\n",
    "    --model_script=model/script.pt \\\n",
    "    --encoder=model/encoder.pt \\\n",
    "    --decoder=model/decoder.pt \\\n",
    "    --positional_encoding=model/positional_encoding.pt \\\n",
    "    --generator=model/generator.pt \\\n",
    "    --src_tok_emb=model/src_tok_emb.pt \\\n",
    "    --tgt_tok_emb=model/tgt_tok_emb.pt \\\n",
    "    --vocab_src=model/vocab_src.txt \\\n",
    "    --vocab_tgt=model/vocab_tgt.txt \\\n",
    "    --params=model/params.json \\\n",
    "    --device=cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
