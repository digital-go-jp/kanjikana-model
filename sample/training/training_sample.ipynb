{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIモデル訓練サンプル\n",
    "PythonでEng-Fraサンプルデータを用いて，FrenchからEnglishへ翻訳するモデルを訓練する。\n",
    "本モデルでは、単語単位ではなく、文字単位で分割し、学習します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## プログラム説明\n",
    "\n",
    "word_model.py\n",
    "\n",
    "内部で，pytorchのtransformerライブラリを呼び出している。また，入出力の文字列の分割は，NLTKライブラリを用いて，単語単位に分割している。\n",
    "各パラメタの詳細は [Pytorch](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html) を参照のこと。\n",
    "\n",
    "- emb_size    \n",
    "  単語のエンベッドサイズ\n",
    "- nhead    \n",
    "  transformerのMultiHeadAttentionのヘッド数\n",
    "- ffn_hid_dim       \n",
    "  FeedForwardNeuralNetworkの次元数\n",
    "- batch_size     \n",
    "  ミニバッチサイズ。メモリが足りないときや計算速度を早めたいときにはこのサイズを変更する\n",
    "- num_encoder_layers    \n",
    "  エンコーダ内のサブエンコーダ層の数 \n",
    "- num_decoder_layers    \n",
    "  デコーダ内のサブデコーダ層の数\n",
    "- lr   \n",
    "  学習率\n",
    "- dropout    \n",
    "  ドロップアウトの割合，1=100%\n",
    "- num_epochs    \n",
    "  学習用データを何周学習するか\n",
    "- device    \n",
    "  cuda: Cudaが使えるマシンではこれを選択\n",
    "  mps: Apple Silicornが使えるマシンではこれを選択\n",
    "  cpu: CPUで計算\n",
    "- earlystop_patient    \n",
    "  num_epochs以下でも，開発用データで，Lossが下がらなくなった回数がearlystop_patientより大きくなると，計算を終了させる\n",
    "- output_dir    \n",
    "  学習したモデルを格納するディレクトリ。ディレクトリには checkpoint_xxx.pt（xxxはepoch数）とcheckpoint_best.ptが作成され，valid lossが最も小さくなったepoch回のモデルをcheckpoint_best.ptとして保存する\n",
    "- tensorboard_logdir    \n",
    "  tensorboard のログを格納するディレクトリ。学習結果などを視覚化して表示できる。 tensorboard --logdir tensorboard_logdir で起動し，http://localhost:6006でアクセスすると表示される\n",
    "- prefix    \n",
    "  jsonl形式の訓練データ及び開発データのprefix\n",
    "- source_lang    \n",
    "  jsonl形式の訓練データ及び開発データでの，翻訳元となるデータにつけるキー\n",
    "- target_lang     \n",
    "  jsonl形式の訓練データ及び開発データでの，翻訳先となるデータにつけるキー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/analysis01/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/analysis01/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "num_epochs:100\n",
      "Epoch: 1, Train loss: 5.722, Val loss: 8.062, Epoch time = 1208.490s\n",
      "Epoch: 2, Train loss: 5.106, Val loss: 10.159, Epoch time = 1209.342s\n",
      "Epoch: 3, Train loss: 4.899, Val loss: 9.065, Epoch time = 1205.414s\n",
      "Epoch: 4, Train loss: 4.672, Val loss: 7.804, Epoch time = 1210.727s\n",
      "Epoch: 5, Train loss: 4.561, Val loss: 6.362, Epoch time = 1208.014s\n",
      "Epoch: 6, Train loss: 4.488, Val loss: 5.876, Epoch time = 1209.898s\n",
      "Epoch: 7, Train loss: 4.410, Val loss: 5.306, Epoch time = 1206.584s\n",
      "Epoch: 8, Train loss: 4.252, Val loss: 4.655, Epoch time = 1215.643s\n",
      "Epoch: 9, Train loss: 4.078, Val loss: 4.421, Epoch time = 1205.246s\n",
      "Epoch: 10, Train loss: 3.989, Val loss: 4.280, Epoch time = 1214.007s\n",
      "Epoch: 11, Train loss: 3.894, Val loss: 4.091, Epoch time = 1208.137s\n",
      "Epoch: 12, Train loss: 3.792, Val loss: 4.028, Epoch time = 1209.832s\n",
      "Epoch: 13, Train loss: 3.709, Val loss: 3.944, Epoch time = 1205.404s\n",
      "Epoch: 14, Train loss: 3.633, Val loss: 3.892, Epoch time = 1206.442s\n",
      "Epoch: 15, Train loss: 3.565, Val loss: 3.732, Epoch time = 1210.385s\n",
      "Epoch: 16, Train loss: 3.497, Val loss: 3.548, Epoch time = 1207.079s\n",
      "Epoch: 17, Train loss: 3.432, Val loss: 3.512, Epoch time = 1208.000s\n",
      "Epoch: 18, Train loss: 3.368, Val loss: 3.462, Epoch time = 1209.082s\n",
      "Epoch: 19, Train loss: 3.301, Val loss: 3.295, Epoch time = 1205.451s\n",
      "Epoch: 20, Train loss: 3.235, Val loss: 3.203, Epoch time = 1205.551s\n",
      "Epoch: 21, Train loss: 3.170, Val loss: 3.123, Epoch time = 1205.968s\n",
      "Epoch: 22, Train loss: 3.108, Val loss: 3.077, Epoch time = 1207.372s\n",
      "Epoch: 23, Train loss: 3.052, Val loss: 3.053, Epoch time = 1206.038s\n",
      "Epoch: 24, Train loss: 2.999, Val loss: 3.001, Epoch time = 1201.465s\n",
      "Epoch: 25, Train loss: 2.947, Val loss: 2.923, Epoch time = 1203.782s\n",
      "Epoch: 26, Train loss: 2.895, Val loss: 2.869, Epoch time = 1203.152s\n",
      "Epoch: 27, Train loss: 2.844, Val loss: 2.816, Epoch time = 1205.147s\n",
      "Epoch: 28, Train loss: 2.795, Val loss: 2.783, Epoch time = 1204.437s\n",
      "Epoch: 29, Train loss: 2.749, Val loss: 2.750, Epoch time = 1204.671s\n",
      "Epoch: 30, Train loss: 2.705, Val loss: 2.694, Epoch time = 1203.459s\n",
      "Epoch: 31, Train loss: 2.664, Val loss: 2.653, Epoch time = 1204.792s\n",
      "Epoch: 32, Train loss: 2.622, Val loss: 2.632, Epoch time = 1201.912s\n",
      "Epoch: 33, Train loss: 2.583, Val loss: 2.613, Epoch time = 1208.767s\n",
      "Epoch: 34, Train loss: 2.544, Val loss: 2.554, Epoch time = 1202.876s\n",
      "Epoch: 35, Train loss: 2.505, Val loss: 2.510, Epoch time = 1207.576s\n",
      "Epoch: 36, Train loss: 2.470, Val loss: 2.463, Epoch time = 1202.220s\n",
      "Epoch: 37, Train loss: 2.434, Val loss: 2.450, Epoch time = 1203.929s\n",
      "Epoch: 38, Train loss: 2.398, Val loss: 2.392, Epoch time = 1205.193s\n",
      "Epoch: 39, Train loss: 2.363, Val loss: 2.382, Epoch time = 1201.826s\n",
      "Epoch: 40, Train loss: 2.328, Val loss: 2.335, Epoch time = 1203.988s\n",
      "Epoch: 41, Train loss: 2.295, Val loss: 2.316, Epoch time = 1203.480s\n",
      "Epoch: 42, Train loss: 2.262, Val loss: 2.284, Epoch time = 1203.087s\n",
      "Epoch: 43, Train loss: 2.228, Val loss: 2.256, Epoch time = 1205.037s\n",
      "Epoch: 44, Train loss: 2.197, Val loss: 2.220, Epoch time = 1202.904s\n",
      "Epoch: 45, Train loss: 2.166, Val loss: 2.199, Epoch time = 1203.270s\n",
      "Epoch: 46, Train loss: 2.135, Val loss: 2.175, Epoch time = 1204.433s\n",
      "Epoch: 47, Train loss: 2.105, Val loss: 2.146, Epoch time = 1207.594s\n",
      "Epoch: 48, Train loss: 2.075, Val loss: 2.138, Epoch time = 1205.038s\n",
      "Epoch: 49, Train loss: 2.046, Val loss: 2.103, Epoch time = 1206.563s\n",
      "Epoch: 50, Train loss: 2.016, Val loss: 2.081, Epoch time = 1204.551s\n",
      "Epoch: 51, Train loss: 1.988, Val loss: 2.057, Epoch time = 1229.390s\n",
      "Epoch: 52, Train loss: 1.961, Val loss: 2.033, Epoch time = 1238.167s\n",
      "Epoch: 53, Train loss: 1.934, Val loss: 2.018, Epoch time = 1238.054s\n",
      "Epoch: 54, Train loss: 1.908, Val loss: 1.971, Epoch time = 1243.186s\n",
      "Epoch: 55, Train loss: 1.882, Val loss: 1.999, Epoch time = 1238.524s\n",
      "Epoch: 56, Train loss: 1.858, Val loss: 1.939, Epoch time = 1243.548s\n",
      "Epoch: 57, Train loss: 1.833, Val loss: 1.921, Epoch time = 1243.393s\n",
      "Epoch: 58, Train loss: 1.807, Val loss: 1.904, Epoch time = 1238.497s\n",
      "Epoch: 59, Train loss: 1.784, Val loss: 1.876, Epoch time = 1238.752s\n",
      "Epoch: 60, Train loss: 1.762, Val loss: 1.865, Epoch time = 1238.170s\n",
      "Epoch: 61, Train loss: 1.737, Val loss: 1.859, Epoch time = 1236.297s\n",
      "Epoch: 62, Train loss: 1.716, Val loss: 1.824, Epoch time = 1238.080s\n",
      "Epoch: 63, Train loss: 1.693, Val loss: 1.817, Epoch time = 1241.426s\n",
      "Epoch: 64, Train loss: 1.672, Val loss: 1.801, Epoch time = 1237.273s\n",
      "Epoch: 65, Train loss: 1.650, Val loss: 1.777, Epoch time = 1235.499s\n",
      "Epoch: 66, Train loss: 1.629, Val loss: 1.775, Epoch time = 1236.668s\n",
      "Epoch: 67, Train loss: 1.609, Val loss: 1.750, Epoch time = 1243.183s\n",
      "Epoch: 68, Train loss: 1.590, Val loss: 1.755, Epoch time = 1238.325s\n",
      "Epoch: 69, Train loss: 1.567, Val loss: 1.730, Epoch time = 1238.542s\n",
      "Epoch: 70, Train loss: 1.548, Val loss: 1.729, Epoch time = 1236.561s\n",
      "Epoch: 71, Train loss: 1.530, Val loss: 1.705, Epoch time = 1238.586s\n",
      "Epoch: 72, Train loss: 1.511, Val loss: 1.678, Epoch time = 1236.702s\n",
      "Epoch: 73, Train loss: 1.492, Val loss: 1.683, Epoch time = 1220.083s\n",
      "Epoch: 74, Train loss: 1.472, Val loss: 1.656, Epoch time = 1205.617s\n",
      "Epoch: 75, Train loss: 1.453, Val loss: 1.657, Epoch time = 1206.972s\n",
      "Epoch: 76, Train loss: 1.435, Val loss: 1.655, Epoch time = 1206.296s\n",
      "Epoch: 77, Train loss: 1.416, Val loss: 1.632, Epoch time = 1205.298s\n",
      "Epoch: 78, Train loss: 1.400, Val loss: 1.618, Epoch time = 1211.532s\n",
      "Epoch: 79, Train loss: 1.382, Val loss: 1.614, Epoch time = 1211.080s\n",
      "Epoch: 80, Train loss: 1.365, Val loss: 1.610, Epoch time = 1210.857s\n",
      "Epoch: 81, Train loss: 1.349, Val loss: 1.587, Epoch time = 1208.115s\n",
      "Epoch: 82, Train loss: 1.331, Val loss: 1.583, Epoch time = 1209.444s\n",
      "Epoch: 83, Train loss: 1.315, Val loss: 1.566, Epoch time = 1207.650s\n",
      "Epoch: 84, Train loss: 1.297, Val loss: 1.559, Epoch time = 1212.485s\n",
      "Epoch: 85, Train loss: 1.281, Val loss: 1.547, Epoch time = 1206.621s\n",
      "Epoch: 86, Train loss: 1.264, Val loss: 1.545, Epoch time = 1212.718s\n",
      "Epoch: 87, Train loss: 1.249, Val loss: 1.525, Epoch time = 1206.822s\n",
      "Epoch: 88, Train loss: 1.233, Val loss: 1.514, Epoch time = 1207.805s\n",
      "Epoch: 89, Train loss: 1.218, Val loss: 1.519, Epoch time = 1204.250s\n",
      "Epoch: 90, Train loss: 1.202, Val loss: 1.505, Epoch time = 1206.276s\n",
      "Epoch: 91, Train loss: 1.187, Val loss: 1.495, Epoch time = 1209.579s\n",
      "Epoch: 92, Train loss: 1.172, Val loss: 1.484, Epoch time = 1206.487s\n",
      "Epoch: 93, Train loss: 1.157, Val loss: 1.473, Epoch time = 1205.097s\n",
      "Epoch: 94, Train loss: 1.143, Val loss: 1.472, Epoch time = 1205.524s\n",
      "Epoch: 95, Train loss: 1.127, Val loss: 1.460, Epoch time = 1204.688s\n",
      "Epoch: 96, Train loss: 1.115, Val loss: 1.452, Epoch time = 1206.203s\n",
      "Epoch: 97, Train loss: 1.098, Val loss: 1.440, Epoch time = 1209.965s\n",
      "Epoch: 98, Train loss: 1.085, Val loss: 1.430, Epoch time = 1205.424s\n",
      "Epoch: 99, Train loss: 1.072, Val loss: 1.440, Epoch time = 1204.416s\n",
      "Epoch: 100, Train loss: 1.057, Val loss: 1.421, Epoch time = 1205.887s\n"
     ]
    }
   ],
   "source": [
    "# Pythonで訓練をする\n",
    "# deviceはcuda or cpu or mps\n",
    "#  mps: apple silicon\n",
    "# out of memoryが発生した際には、batch_sizeを減らす\n",
    "\n",
    "\n",
    "# 途中から計算するときには，modelディレクトリに checkpoint_xxx.pt(xxxは計算済みのepoch数)とcheckpoint_best.pt が存在すること\n",
    "\n",
    "!python ./word_model.py \\\n",
    "  --emb_size 1024 \\\n",
    "  --nhead 8 \\\n",
    "  --ffn_hid_dim 2048 \\\n",
    "  --batch_size 32 \\\n",
    "  --num_encoder_layers 12 \\\n",
    "  --num_decoder_layers 12 \\\n",
    "  --lr 0.00002 \\\n",
    "  --dropout 0.3 \\\n",
    "  --num_epochs 100 \\\n",
    "  --device cuda \\\n",
    "  --earlystop_patient 3 \\\n",
    "  --output_dir model \\\n",
    "  --tensorboard_logdir logs \\\n",
    "  --prefix translation \\\n",
    "  --source_lang fra \\\n",
    "  --target_lang eng \\\n",
    "  --train_file ../dataset/train.jsonl \\\n",
    "  --valid_file ../dataset/val.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# javaのDJLで使えるようにモデルファイルを変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/analysis01/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/analysis01/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 435360), started 0:00:04 ago. (Use '!kill 435360' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9932f15156cbce30\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9932f15156cbce30\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## javaのDJLで使えるようにモデルファイルを変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/analysis01/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/analysis01/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!python convert.py  \\\n",
    "    --model_file=model/checkpoint_best.pt \\\n",
    "    --model_script=model/script.pt \\\n",
    "    --encoder=model/encoder.pt \\\n",
    "    --decoder=model/decoder.pt \\\n",
    "    --positional_encoding=model/positional_encoding.pt \\\n",
    "    --generator=model/generator.pt \\\n",
    "    --src_tok_emb=model/src_tok_emb.pt \\\n",
    "    --tgt_tok_emb=model/tgt_tok_emb.pt \\\n",
    "    --vocab_src=model/vocab_src.txt \\\n",
    "    --vocab_tgt=model/vocab_tgt.txt \\\n",
    "    --params=model/params.json \\\n",
    "    --device=cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
