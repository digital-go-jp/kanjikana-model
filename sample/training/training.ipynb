{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIモデル訓練サンプル\n",
    "PythonでEng-Fraサンプルデータを用いて，FrenchからEnglishへ翻訳するモデルを訓練する。\n",
    "本モデルでは、単語単位ではなく、文字単位で分割し、学習します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## プログラム説明\n",
    "\n",
    "model.py\n",
    "\n",
    "内部で，pytorchのtransformerライブラリを呼び出している。各パラメタの詳細は [Pytorch](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html) を参照のこと。\n",
    "\n",
    "- emb_size    \n",
    "  単語のエンベッドサイズ\n",
    "- nhead    \n",
    "  transformerのMultiHeadAttentionのヘッド数\n",
    "- ffn_hid_dim       \n",
    "  FeedForwardNeuralNetworkの次元数\n",
    "- batch_size     \n",
    "  ミニバッチサイズ。メモリが足りないときや計算速度を早めたいときにはこのサイズを変更する\n",
    "- num_encoder_layers    \n",
    "  エンコーダ内のサブエンコーダ層の数 \n",
    "- num_decoder_layers    \n",
    "  デコーダ内のサブデコーダ層の数\n",
    "- lr   \n",
    "  学習率\n",
    "- dropout    \n",
    "  ドロップアウトの割合，1=100%\n",
    "- num_epochs    \n",
    "  学習用データを何周学習するか\n",
    "- device    \n",
    "  cuda: Cudaが使えるマシンではこれを選択\n",
    "  mps: Apple Silicornが使えるマシンではこれを選択\n",
    "  cpu: CPUで計算\n",
    "- earlystop_patient    \n",
    "  num_epochs以下でも，開発用データで，Lossが下がらなくなった回数がearlystop_patientより大きくなると，計算を終了させる\n",
    "- output_dir    \n",
    "  学習したモデルを格納するディレクトリ。ディレクトリには checkpoint_xxx.pt（xxxはepoch数）とcheckpoint_best.ptが作成され，valid lossが最も小さくなったepoch回のモデルをcheckpoint_best.ptとして保存する\n",
    "- tensorboard_logdir    \n",
    "  tensorboard のログを格納するディレクトリ。学習結果などを視覚化して表示できる。 tensorboard --logdir tensorboard_logdir で起動し，http://localhost:6006でアクセスすると表示される\n",
    "- prefix    \n",
    "  jsonl形式の訓練データ及び開発データのprefix\n",
    "- source_lang    \n",
    "  jsonl形式の訓練データ及び開発データでの，翻訳元となるデータにつけるキー\n",
    "- target_lang     \n",
    "  jsonl形式の訓練データ及び開発データでの，翻訳先となるデータにつけるキー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epochs:100\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/analysis01/src/kanjikana-model/sample/training/./model.py\", line 486, in <module>\n",
      "    main()\n",
      "  File \"/home/analysis01/src/kanjikana-model/sample/training/./model.py\", line 482, in main\n",
      "    KanjiKanaTransformer(args).train()\n",
      "  File \"/home/analysis01/src/kanjikana-model/sample/training/./model.py\", line 427, in train\n",
      "    train_loss = self.train_epoch(train_iter, transformer, optimizer, loss_fn)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/analysis01/src/kanjikana-model/sample/training/./model.py\", line 242, in train_epoch\n",
      "    logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/analysis01/src/kanjikana-model/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/analysis01/src/kanjikana-model/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/analysis01/src/kanjikana-model/sample/training/./model.py\", line 125, in forward\n",
      "    outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/analysis01/src/kanjikana-model/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/analysis01/src/kanjikana-model/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/analysis01/src/kanjikana-model/venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 278, in forward\n",
      "    output = self.decoder(\n",
      "             ^^^^^^^^^^^^^\n",
      "  File \"/home/analysis01/src/kanjikana-model/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/analysis01/src/kanjikana-model/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/analysis01/src/kanjikana-model/venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 602, in forward\n",
      "    output = mod(\n",
      "             ^^^^\n",
      "  File \"/home/analysis01/src/kanjikana-model/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/analysis01/src/kanjikana-model/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/analysis01/src/kanjikana-model/venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 1087, in forward\n",
      "    x + self._sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/analysis01/src/kanjikana-model/venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 1107, in _sa_block\n",
      "    x = self.self_attn(\n",
      "        ^^^^^^^^^^^^^^^\n",
      "  File \"/home/analysis01/src/kanjikana-model/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/analysis01/src/kanjikana-model/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/analysis01/src/kanjikana-model/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py\", line 1368, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/analysis01/src/kanjikana-model/venv/lib/python3.11/site-packages/torch/nn/functional.py\", line 6097, in multi_head_attention_forward\n",
      "    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/analysis01/src/kanjikana-model/venv/lib/python3.11/site-packages/torch/nn/functional.py\", line 5501, in _in_projection_packed\n",
      "    proj = linear(q, w, b)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 14.62 GiB of which 63.38 MiB is free. Including non-PyTorch memory, this process has 14.55 GiB memory in use. Of the allocated memory 13.77 GiB is allocated by PyTorch, and 663.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "# Pythonで訓練をする\n",
    "# deviceはcuda or cpu or mps\n",
    "#  mps: apple silicon\n",
    "# out of memoryが発生した際には、batch_sizeを減らす\n",
    "\n",
    "\n",
    "# 途中から計算するときには，modelディレクトリに checkpoint_xxx.pt(xxxは計算済みのepoch数)とcheckpoint_best.pt が存在すること\n",
    "\n",
    "!python ./model.py \\\n",
    "  --emb_size 1024 \\\n",
    "  --nhead 8 \\\n",
    "  --ffn_hid_dim 2048 \\\n",
    "  --batch_size 32 \\\n",
    "  --num_encoder_layers 12 \\\n",
    "  --num_decoder_layers 12 \\\n",
    "  --lr 0.00002 \\\n",
    "  --dropout 0.3 \\\n",
    "  --num_epochs 100 \\\n",
    "  --device cuda \\\n",
    "  --earlystop_patient 3 \\\n",
    "  --output_dir model \\\n",
    "  --tensorboard_logdir logs \\\n",
    "  --prefix translation \\\n",
    "  --source_lang fra \\\n",
    "  --target_lang eng \\\n",
    "  --train_file ../dataset/train.jsonl \\\n",
    "  --valid_file ../dataset/val.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# javaのDJLで使えるようにモデルファイルを変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/utsubo-katsuhiko/Documents/GitHub/kanjikana-model/sample/training/convert.py\", line 75, in <module>\n",
      "    main()\n",
      "  File \"/Users/utsubo-katsuhiko/Documents/GitHub/kanjikana-model/sample/training/convert.py\", line 70, in main\n",
      "    KanjiKanaTransformerScripted(args).convert()\n",
      "  File \"/Users/utsubo-katsuhiko/Documents/GitHub/kanjikana-model/sample/training/convert.py\", line 19, in convert\n",
      "    best_checkpoint = torch.load(self.args.model_file, map_location=torch.device(self.args.device))\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/utsubo-katsuhiko/Documents/GitHub/kanjikana-model/venv/lib/python3.12/site-packages/torch/serialization.py\", line 1319, in load\n",
      "    with _open_file_like(f, \"rb\") as opened_file:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/utsubo-katsuhiko/Documents/GitHub/kanjikana-model/venv/lib/python3.12/site-packages/torch/serialization.py\", line 659, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/utsubo-katsuhiko/Documents/GitHub/kanjikana-model/venv/lib/python3.12/site-packages/torch/serialization.py\", line 640, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "                     ^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'model/checkpoint_best.pt'\n"
     ]
    }
   ],
   "source": [
    "!python convert.py  \\\n",
    "    --model_file=model/checkpoint_best.pt \\\n",
    "    --model_script=model/script.pt \\\n",
    "    --encoder=model/encoder.pt \\\n",
    "    --decoder=model/decoder.pt \\\n",
    "    --positional_encoding=model/positional_encoding.pt \\\n",
    "    --generator=model/generator.pt \\\n",
    "    --src_tok_emb=model/src_tok_emb.pt \\\n",
    "    --tgt_tok_emb=model/tgt_tok_emb.pt \\\n",
    "    --vocab_src=model/vocab_src.txt \\\n",
    "    --vocab_tgt=model/vocab_tgt.txt \\\n",
    "    --params=model/params.json \\\n",
    "    --device=cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
